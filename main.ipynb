{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "febc8ed225c173224913a33e4bcc13b4e98623c14b3011f9df708a037d5d8a22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Community Prediction based on Taxi Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load dataset\n",
    "1. TRIP_ID: (String) It contains an unique identifier for each trip;\n",
    "1. CALL_TYPE: (char) It identifies the way used to demand this service. It may contain one of three possible values:\n",
    "    - ‘A’ if this trip was dispatched from the central;\n",
    "    - ‘B’ if this trip was demanded directly to a taxi driver on a specific stand;\n",
    "    - ‘C’ otherwise (i.e. a trip demanded on a random street).\n",
    "1. ORIGIN_CALL: (integer) It contains an unique identifier for each phone number which was used to demand, at least, one service. It identifies the trip’s customer if CALL_TYPE=’A’. Otherwise, it assumes a NULL value;\n",
    "1. ORIGIN_STAND: (integer): It contains an unique identifier for the taxi stand. It identifies the starting point of the trip if CALL_TYPE=’B’. Otherwise, it assumes a NULL value;\n",
    "1. TAXI_ID: (integer): It contains an unique identifier for the taxi driver that performed each trip;\n",
    "1. TIMESTAMP: (integer) Unix Timestamp (in seconds). It identifies the trip’s start; \n",
    "1. DAYTYPE: (char) It identifies the daytype of the trip’s start. It assumes one of three possible values:\n",
    "    - ‘B’ if this trip started on a holiday or any other special day (i.e. extending holidays, floating holidays, etc.);\n",
    "    - ‘C’ if the trip started on a day before a type-B day;\n",
    "    - ‘A’ otherwise (i.e. a normal day, workday or weekend).\n",
    "1. MISSING_DATA: (Boolean) It is FALSE when the GPS data stream is complete and TRUE whenever one (or more) locations are missing\n",
    "1. POLYLINE: (String): It contains a list of GPS coordinates (i.e. WGS84 format) mapped as a string. The beginning and the end of the string are identified with brackets (i.e. \\[ and \\], respectively). Each pair of coordinates is also identified by the same brackets as \\[LONGITUDE, LATITUDE\\]. This list contains one pair of coordinates for each 15 seconds of trip. The last list item corresponds to the trip’s destination while the first one represents its start;\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Iterator\n",
    "\n",
    "enum_mapping = {'A': 1, 'B': 2, 'C': 3}\n",
    "\n",
    "def load_csv_content() -> Iterator:\n",
    "    '''Returns a generator for all lines in the csv file with correct field types.'''\n",
    "    \n",
    "    with open('input/train.csv') as csv_file:\n",
    "        reader = csv.reader(csv_file)    \n",
    "\n",
    "        headers = [h.lower() for h in next(reader)]\n",
    "\n",
    "        for line in reader:\n",
    "            # convert line fields to correct type\n",
    "            for i in range(len(headers)):\n",
    "                # trip_id AS string\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                # call_type, day_type \n",
    "                if i in [1, 6]:\n",
    "                    line[i] = enum_mapping[line[i]]\n",
    "                # origin_call, origin_stand, taxi_id AS int\n",
    "                elif i in [2, 3, 4]:\n",
    "                    line[i] = int(line[i]) if line[i] != \"\" else \"\"\n",
    "                # timestamp AS timestamp\n",
    "                elif i == 5:\n",
    "                    # datetime is not serializable\n",
    "                    # line[i] = datetime.fromtimestamp(int(line[i]))\n",
    "                    line[i] = int(line[i])\n",
    "                # missing_data AS bool\n",
    "                elif i == 7: \n",
    "                    line[i] = line[i].lower() == 'true'\n",
    "                # polyline AS List[List[float]]\n",
    "                elif i == 8:\n",
    "                    line[i] = json.loads(line[i])\n",
    "\n",
    "            entry = dict(zip(headers, line))\n",
    "            yield entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(load_csv_content()))"
   ]
  },
  {
   "source": [
    "## Display some dataset routes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import folium\n",
    "\n",
    "def displayNodes(nodes: List[List[float]]):  \n",
    "    '''\n",
    "    Displays the nodes on a map of the city.\n",
    "\n",
    "    :param nodes: A list of coordinates, eg. [[1,2],[1,3]]\n",
    "    '''\n",
    "    m = folium.Map(location=[41.15,-8.6],tiles='stamenterrain',zoom_start=12, control_scale=True) \n",
    "\n",
    "    for idx, node in enumerate(nodes): \n",
    "        popupLabel = idx\n",
    "\n",
    "        folium.Marker(\n",
    "          location=[node[1], node[0]],\n",
    "          #popup='Cluster Nr: '+ str(node.cluster_no),\n",
    "          popup=popupLabel,\n",
    "          icon=folium.Icon(color='red', icon='circle'),\n",
    "        ).add_to(m)\n",
    "      \n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_csv_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayNodes(next(content)['polyline'])"
   ]
  },
  {
   "source": [
    "# Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split dataset into multiple layers\n",
    "The SMART pipeline is used to split up the data in multiple layers. Therefore, the csv file is uploaded to the Semantic Linking microservice for layer creation. <br />\n",
    "Next, the Role Stage Discovery microservice will cluster the individual layers and splits them into multiple time windows based on the timestamp.\n",
    "\n",
    "## Define features per cluster/ layer\n",
    "### \"Local\" features based on single clusters\n",
    "- cluster size ($\\#\\ cluster\\ nodes$)\n",
    "- cluster standard deviation (variance from cluster mean)\n",
    "- cluster scarcity (ratio $\\frac{cluster\\ range}{cluster\\ size}$ ) <br />\n",
    "Scarcity is perferred over density to avoid divide-by-zero error\n",
    "- (cluster trustworthiness)\n",
    "### \"Global\" features based on clusters in the context of a layer\n",
    "- cluster importance I (ratio $\\frac{cluster\\ size}{\\#\\ layer\\ nodes}$)\n",
    "- cluster importance II (ratio $\\frac{1}{diversity}$, where *diversity* = number of clusters with nodes > 0)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Calculate the Metrics for the Clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "from entities import TimeWindow, Cluster\n",
    "\n",
    "def calculate_metrics_for_clusters(layer_name: str = 'CallTypeLayer', feature_names: List[str] = ['call_type']):\n",
    "    print(f\"Working on {layer_name}\")\n",
    "\n",
    "    path_in = f'input/timeslices/{layer_name}'\n",
    "    path_out = f'input/metrics/{layer_name}.json'\n",
    "\n",
    "    complete_clusters: List[Cluster] = []\n",
    "\n",
    "    for root, _, files in os.walk(path_in):\n",
    "        for f in files:\n",
    "            with open(os.path.join(root, f), 'r') as file:\n",
    "                json_slice = json.loads(file.read())\n",
    "                time_window = TimeWindow.create_from_serializable_dict(json_slice)\n",
    "\n",
    "                # create all clusters + metrics for one time window\n",
    "                clusters = Cluster.create_multiple_from_time_window(time_window, feature_names)\n",
    "                complete_clusters.extend(clusters)\n",
    "        \n",
    "    # store the cluster metrics\n",
    "    with open(path_out, 'w') as file:\n",
    "        file.write(json.dumps([cl.__dict__ for cl in complete_clusters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ['CallTypeLayer', 'call_type'],\n",
    "    ['DayTypeLayer', 'day_type'],\n",
    "    ['TaxiIdLayer', 'taxi_id'],\n",
    "\n",
    "    ['OriginCallLayer', ('call_type', 'origin_call')],\n",
    "    ['OriginStandLayer', ('call_type', 'origin_stand')],\n",
    "    ['StartLocationLayer', ('start_location_lat', 'start_location_long')],\n",
    "    ['EndLocationLayer', ('end_location_lat', 'end_location_long')],\n",
    "]\n",
    "\n",
    "for layer in layers:\n",
    "    calculate_metrics_for_clusters(layer[0], layer[1])"
   ]
  },
  {
   "source": [
    "## Prepare cluster metrics and evolution labels for ML"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example how to convert time to a cyclic 2d feature\n",
    "\n",
    "MAX_TIME_VAL = 52 # for weeks\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "times = np.asarray([i+1 for i in range(52)][::])\n",
    "\n",
    "df = {}\n",
    "df['sin_time'] = np.sin(2*np.pi*times/MAX_TIME_VAL)\n",
    "df['cos_time'] = np.cos(2*np.pi*times/MAX_TIME_VAL)\n",
    "\n",
    "plt.plot(df['sin_time'])\n",
    "plt.plot(df['cos_time'])\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df['sin_time'], df['cos_time'])\n",
    "plt.show()\n",
    "\n",
    "# feature_new = {i+1:(s,c) for i,(s,c) in enumerate(zip(df['sin_time'], df['cos_time']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example how to calculate convex hull from points\n",
    "# Used to calculate 2d area for Scarcity metric\n",
    "\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "import numpy as np\n",
    "\n",
    "points = np.asarray([[0.0,0.0], [1.0,3.0], [3.0,2.0], [0.0,2.0], [1.0,2.0], [2.0,2.0], [2.0,1.0]])\n",
    "\n",
    "def _get_polygon_border_points(points) -> 'np.array':\n",
    "        hull = ConvexHull(points)\n",
    "        return points[hull.vertices]\n",
    "\n",
    "res = _get_polygon_border_points(points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], 'o')\n",
    "plt.plot(res[:,0], res[:,1], 'o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from entities import Cluster\n",
    "import collections\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "def get_evolution_label(old_size: int, new_size: int) -> int:\n",
    "    '''Returns the evolution label as int by mapping 0..4 to {continuing, shrinking, growing, dissolving, forming}.'''\n",
    "    if old_size == new_size:\n",
    "        return 0 # continuing\n",
    "    if old_size == 0 and new_size != 0:\n",
    "        return 4 # forming\n",
    "    if old_size != 0 and new_size == 0:\n",
    "        return 3 # dissolving\n",
    "    if old_size > new_size:\n",
    "        return 1 # shrinking\n",
    "    if old_size < new_size:\n",
    "        return 2 # growing\n",
    "\n",
    "def get_cyclic_time_feature(time: int, max_time_value: int = 52) -> (float, float):\n",
    "    return (np.sin(2*np.pi*time/max_time_value),\n",
    "            np.cos(2*np.pi*time/max_time_value))\n",
    "\n",
    "def create_metrics_training_data(N: int = 3, layer_name: str = 'CallTypeLayer') -> Iterable:\n",
    "    \"\"\"\n",
    "    A single metrics training data point should look like this:\n",
    "\n",
    "    (cluster_size, cluster_std_dev, cluster_scarcity, cluster_import1, cluster_import2, time_info) ^ N, evolution_label\n",
    "    time_info ... the time as 2d cyclic feature, i.e. time_info := (time_f1, time_f2)\n",
    "\n",
    "    The first tuple represents metrics from the cluster in t_i-(N-1).\n",
    "    The Nth tuple represents metrics from the cluster in t_i.\n",
    "    The label is one of {continuing, shrinking, growing, dissolving, forming} \\ {splitting, merging} and identifies the change for t_i+1.\n",
    "    \n",
    "    :param N: number of cluster metric tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    path_in = f\"input/metrics/{layer_name}.json\"\n",
    "    with open(path_in, 'r') as file:\n",
    "        data = [Cluster.create_from_dict(cl_d) for cl_d in json.loads(file.read())]\n",
    "\n",
    "    data.sort(key=lambda cl: (cl.cluster_id, cl.time_window_id))\n",
    "\n",
    "    # manually prepare deque with N metric_tuples + evolution label\n",
    "    tuples = []\n",
    "    prev_cluster_id = -1\n",
    "\n",
    "    for i, cur_cluster in enumerate(data[:-1]):\n",
    "\n",
    "        if cur_cluster.cluster_id != data[i+1].cluster_id:\n",
    "            # next cluster slice in list will be another cluster id -> restart deque and skip adding the current (last) cluster slice\n",
    "            tuples = []\n",
    "            continue\n",
    "\n",
    "        cur_metrics = (cur_cluster.size, cur_cluster.std_dev, cur_cluster.scarcity, cur_cluster.importance1, cur_cluster.importance2, get_cyclic_time_feature(cur_cluster.get_time_info()))\n",
    "\n",
    "        # deque function: adding N+1st element will remove oldest one\n",
    "        if len(tuples) == N:\n",
    "            tuples.pop(0)\n",
    "        tuples.append(cur_metrics)\n",
    "\n",
    "        label = get_evolution_label(cur_cluster.size, data[i+1].size)\n",
    "\n",
    "        if len(tuples) == N:\n",
    "            yield list(tuples) + [label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_metrics_datapoint(datapoint: list) -> ('X', 'Y'):\n",
    "    '''\n",
    "    Flattens a single metrics data point in the form:\n",
    "    [(cluster_size, cluster_variance, cluster_density, cluster_import1, cluster_import2, (time_f1, time_f2))^N, evolution_label]\n",
    "    to:\n",
    "    (X: np.array, evolution_label)\n",
    "    '''\n",
    "    flat_list = []\n",
    "    for entry in datapoint[:-1]: # for all x\n",
    "        flat_list.extend(entry[:-1]) # add all number features except the time tuple\n",
    "        flat_list.extend(entry[-1]) # add time tuple\n",
    "\n",
    "    # flat_list.append(datapoint[-1]) # add y\n",
    "\n",
    "    return np.asarray(flat_list), datapoint[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_metrics_data_for_training(data: Iterable) -> ('nparray with Xs', 'nparray with Ys'):\n",
    "    '''Flattens and splits metrics data to match ML conventions.'''\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for element in data:\n",
    "        x, y = flatten_metrics_datapoint(element)\n",
    "        \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    return (np.asarray(X), np.asarray(Y))"
   ]
  },
  {
   "source": [
    "## Evolution Prediction Approach\n",
    "\n",
    "### 1. Prediction of cluster evolution based on metrics from clusters in one layer\n",
    "Use cluster metrics from last N time windows to predict the change in $t_{i+1}$.\n",
    "Either use normal classification with $(cluster\\_metrics)^{N} \\cup (label)$ or choose a RNN.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(create_metrics_training_data(layer_name='CallTypeLayer'))\n",
    "\n",
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "# split in 130 training + 20 testing\n",
    "train_metrics = data[:-30]\n",
    "test_metrics = data[len(data)-30:]\n",
    "\n",
    "print(f\"Working with: {len(train_metrics)} training points + {len(test_metrics)} test points ({len(test_metrics)/(len(train_metrics)+len(test_metrics))}).\")\n",
    "\n",
    "X_train, Y_train = convert_metrics_data_for_training(train_metrics)\n",
    "X_test, Y_test = convert_metrics_data_for_training(test_metrics)\n",
    "\n",
    "\n",
    "import collections\n",
    "import statistics as stat\n",
    "print(f\"Label Occurrences: Total = {collections.Counter(Y_train.tolist() + Y_test.tolist())}, Training = {collections.Counter(Y_train)}, Test = {collections.Counter(Y_test)}\")\n",
    "print(f\"Label Majority Class: Training = {stat.mode(Y_train)}, Test = {stat.mode(Y_test)}\\n\")"
   ]
  },
  {
   "source": [
    "## SVM classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "import sklearn\n",
    "\n",
    "pred_Y = svc.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_true=Y_test, y_pred=pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pickle \n",
    "\n",
    "with open('output/svc.model', 'wb') as file:\n",
    "    b = pickle.dump(svc, file)\n",
    "\n",
    "# import  \n",
    "import pickle \n",
    "\n",
    "FILE_NAME = 'svc'\n",
    "with open(f'output/{FILE_NAME}.model', 'rb') as file:\n",
    "    svc = pickle.load(file)"
   ]
  },
  {
   "source": [
    "## Naive classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import statistics as stat\n",
    "import random\n",
    "\n",
    "def show_majority_class_prediction():\n",
    "    print(\"### Majority Class Prediction: ###\")\n",
    "\n",
    "    majority_class = stat.mode(Y_train)\n",
    "    print(f\"Training majority class = {stat.mode(Y_train)}, Test majority class = {stat.mode(Y_test)}\")\n",
    "\n",
    "    pred_Y = len(Y_test) * [majority_class]\n",
    "    print(sklearn.metrics.classification_report(y_true=Y_test, y_pred=pred_Y))\n",
    "\n",
    "    \n",
    "def show_random_prediction():\n",
    "    print(\"### Random Class Prediction: ###\")\n",
    "\n",
    "    classes = list(set(Y_train))\n",
    "    print(f\"Classes: {classes}\")\n",
    "\n",
    "    pred_Y = random.choices(classes, k=len(Y_test))\n",
    "    print(sklearn.metrics.classification_report(y_true=Y_test, y_pred=pred_Y))\n",
    "\n",
    "\n",
    "show_majority_class_prediction()\n",
    "show_random_prediction()"
   ]
  },
  {
   "source": [
    "### 2. Prediction of cluster evolution based on metrics from cluster interaction between multiple layers\n",
    "*todo*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}