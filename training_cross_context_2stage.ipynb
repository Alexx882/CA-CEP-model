{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0edc7e8da127e731d54753afe8930f1420dc6ae9a13010eedc53dff7bbda352d4",
   "display_name": "Python 3.6.9 64-bit ('venv-gpu2': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "edc7e8da127e731d54753afe8930f1420dc6ae9a13010eedc53dff7bbda352d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Cross-Context Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_case = 'youtube'\n",
    "layer_name = 'DislikesLayer' \n",
    "reference_layer_name = 'ViewsLayer'\n",
    "\n",
    "approach = 'cross_context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "df: DataFrame = pd.read_csv(f'data/{use_case}/ml_input/cross_context/{layer_name}_{reference_layer_name}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nWorking with: 150576 training points + 37644 test points (0.2 test ratio).\nLabel Occurrences: Total = Counter({0.0: 101228, 3.0: 28481, 4.0: 28129, 1.0: 15471, 2.0: 14911}), Training = Counter({0.0: 81021, 3.0: 22736, 4.0: 22524, 1.0: 12359, 2.0: 11936}), Test = Counter({0.0: 20207, 3.0: 5745, 4.0: 5605, 1.0: 3112, 2.0: 2975})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def split_data(dataframe, test_dataset_frac=.2, shuffle=False) -> '(training_data, test_data)':\n",
    "    if shuffle:\n",
    "        dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    training_size = int(len(dataframe) * (1-test_dataset_frac))\n",
    "\n",
    "    train = dataframe[:training_size].reset_index(drop=True)\n",
    "    test = dataframe[training_size:].reset_index(drop=True)\n",
    "\n",
    "    y_train = train[train.columns[-1]]\n",
    "    y_test = test[test.columns[-1]]\n",
    "  \n",
    "    print(f\"\\nWorking with: {len(train)} training points + {len(test)} test points ({len(test)/(len(test)+len(train))} test ratio).\")\n",
    "    print(f\"Label Occurrences: Total = {collections.Counter(y_train.tolist() + y_test.tolist())}, \"\\\n",
    "          f\"Training = {collections.Counter(y_train)}, Test = {collections.Counter(y_test)}\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "training, testing = split_data(df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        n_nodes  n_clusters    entropy  sizes_min  sizes_max  sizes_avg  \\\n",
       "0        9016.0      7037.0  12.629915        1.0        9.0   1.281228   \n",
       "1       10292.0      6955.0  12.573513        1.0       11.0   1.479799   \n",
       "2        9162.0      7015.0  12.624013        1.0        9.0   1.306058   \n",
       "3        9016.0      7037.0  12.629915        1.0        9.0   1.281228   \n",
       "4        9162.0      7015.0  12.624013        1.0        9.0   1.306058   \n",
       "...         ...         ...        ...        ...        ...        ...   \n",
       "150571  10202.0      7002.0  12.584552        1.0       10.0   1.457012   \n",
       "150572   9748.0      7321.0  12.685151        1.0        8.0   1.331512   \n",
       "150573   9729.0      7371.0  12.688347        1.0       10.0   1.319902   \n",
       "150574   9004.0      6856.0  12.590117        1.0        7.0   1.313302   \n",
       "150575   9886.0      7383.0  12.683961        1.0       10.0   1.339022   \n",
       "\n",
       "        sizes_sum  relative_sizes_min  relative_sizes_max  relative_sizes_avg  \\\n",
       "0          9016.0            0.000111            0.000998            0.000142   \n",
       "1         10292.0            0.000097            0.001069            0.000144   \n",
       "2          9162.0            0.000109            0.000982            0.000143   \n",
       "3          9016.0            0.000111            0.000998            0.000142   \n",
       "4          9162.0            0.000109            0.000982            0.000143   \n",
       "...           ...                 ...                 ...                 ...   \n",
       "150571    10202.0            0.000098            0.000980            0.000143   \n",
       "150572     9748.0            0.000103            0.000821            0.000137   \n",
       "150573     9729.0            0.000103            0.001028            0.000136   \n",
       "150574     9004.0            0.000111            0.000777            0.000146   \n",
       "150575     9886.0            0.000101            0.001012            0.000135   \n",
       "\n",
       "        ...  relative_sizes_avg.1  relative_sizes_sum.1  center_dist_min.1  \\\n",
       "0       ...              0.000141                   1.0                0.0   \n",
       "1       ...              0.000143                   1.0                0.0   \n",
       "2       ...              0.000142                   1.0                0.0   \n",
       "3       ...              0.000141                   1.0                0.0   \n",
       "4       ...              0.000142                   1.0                0.0   \n",
       "...     ...                   ...                   ...                ...   \n",
       "150571  ...              0.000144                   1.0                0.0   \n",
       "150572  ...              0.000139                   1.0                0.0   \n",
       "150573  ...              0.000140                   1.0                0.0   \n",
       "150574  ...              0.000151                   1.0                0.0   \n",
       "150575  ...              0.000138                   1.0                0.0   \n",
       "\n",
       "        center_dist_max.1  center_dist_avg.1  center_dist_sum.1     time_f1.1  \\\n",
       "0            6.041375e+04          93.662142       6.650012e+05  6.432491e-16   \n",
       "1            1.531671e+06        1005.152850       7.038080e+06  4.647232e-01   \n",
       "2            1.002264e+06         257.336409       1.810876e+06 -1.205367e-01   \n",
       "3            6.041375e+04          93.662142       6.650012e+05  6.432491e-16   \n",
       "4            1.002264e+06         257.336409       1.810876e+06 -1.205367e-01   \n",
       "...                   ...                ...                ...           ...   \n",
       "150571       2.372543e+06        1149.232271       7.982567e+06  3.546049e-01   \n",
       "150572       1.831231e+06         880.062845       6.342613e+06  1.000000e+00   \n",
       "150573       2.935547e+06        2033.327409       1.453626e+07  8.854560e-01   \n",
       "150574       6.657900e+04         101.640684       6.731662e+05  5.680647e-01   \n",
       "150575       2.176610e+06        1119.616163       8.097064e+06  8.854560e-01   \n",
       "\n",
       "           time_f2.1  cluster_id  evolution_label  \n",
       "0       1.000000e+00      1870.0              0.0  \n",
       "1      -8.854560e-01      3035.0              0.0  \n",
       "2       9.927089e-01      4702.0              1.0  \n",
       "3       1.000000e+00       119.0              0.0  \n",
       "4       9.927089e-01      3332.0              0.0  \n",
       "...              ...         ...              ...  \n",
       "150571 -9.350162e-01      4856.0              0.0  \n",
       "150572 -1.608123e-16      5259.0              3.0  \n",
       "150573  4.647232e-01      3115.0              3.0  \n",
       "150574  8.229839e-01      2253.0              0.0  \n",
       "150575 -4.647232e-01      5573.0              2.0  \n",
       "\n",
       "[150576 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_nodes</th>\n      <th>n_clusters</th>\n      <th>entropy</th>\n      <th>sizes_min</th>\n      <th>sizes_max</th>\n      <th>sizes_avg</th>\n      <th>sizes_sum</th>\n      <th>relative_sizes_min</th>\n      <th>relative_sizes_max</th>\n      <th>relative_sizes_avg</th>\n      <th>...</th>\n      <th>relative_sizes_avg.1</th>\n      <th>relative_sizes_sum.1</th>\n      <th>center_dist_min.1</th>\n      <th>center_dist_max.1</th>\n      <th>center_dist_avg.1</th>\n      <th>center_dist_sum.1</th>\n      <th>time_f1.1</th>\n      <th>time_f2.1</th>\n      <th>cluster_id</th>\n      <th>evolution_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9016.0</td>\n      <td>7037.0</td>\n      <td>12.629915</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.281228</td>\n      <td>9016.0</td>\n      <td>0.000111</td>\n      <td>0.000998</td>\n      <td>0.000142</td>\n      <td>...</td>\n      <td>0.000141</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6.041375e+04</td>\n      <td>93.662142</td>\n      <td>6.650012e+05</td>\n      <td>6.432491e-16</td>\n      <td>1.000000e+00</td>\n      <td>1870.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10292.0</td>\n      <td>6955.0</td>\n      <td>12.573513</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>1.479799</td>\n      <td>10292.0</td>\n      <td>0.000097</td>\n      <td>0.001069</td>\n      <td>0.000144</td>\n      <td>...</td>\n      <td>0.000143</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.531671e+06</td>\n      <td>1005.152850</td>\n      <td>7.038080e+06</td>\n      <td>4.647232e-01</td>\n      <td>-8.854560e-01</td>\n      <td>3035.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9162.0</td>\n      <td>7015.0</td>\n      <td>12.624013</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.306058</td>\n      <td>9162.0</td>\n      <td>0.000109</td>\n      <td>0.000982</td>\n      <td>0.000143</td>\n      <td>...</td>\n      <td>0.000142</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.002264e+06</td>\n      <td>257.336409</td>\n      <td>1.810876e+06</td>\n      <td>-1.205367e-01</td>\n      <td>9.927089e-01</td>\n      <td>4702.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9016.0</td>\n      <td>7037.0</td>\n      <td>12.629915</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.281228</td>\n      <td>9016.0</td>\n      <td>0.000111</td>\n      <td>0.000998</td>\n      <td>0.000142</td>\n      <td>...</td>\n      <td>0.000141</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6.041375e+04</td>\n      <td>93.662142</td>\n      <td>6.650012e+05</td>\n      <td>6.432491e-16</td>\n      <td>1.000000e+00</td>\n      <td>119.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9162.0</td>\n      <td>7015.0</td>\n      <td>12.624013</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.306058</td>\n      <td>9162.0</td>\n      <td>0.000109</td>\n      <td>0.000982</td>\n      <td>0.000143</td>\n      <td>...</td>\n      <td>0.000142</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.002264e+06</td>\n      <td>257.336409</td>\n      <td>1.810876e+06</td>\n      <td>-1.205367e-01</td>\n      <td>9.927089e-01</td>\n      <td>3332.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>150571</th>\n      <td>10202.0</td>\n      <td>7002.0</td>\n      <td>12.584552</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.457012</td>\n      <td>10202.0</td>\n      <td>0.000098</td>\n      <td>0.000980</td>\n      <td>0.000143</td>\n      <td>...</td>\n      <td>0.000144</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.372543e+06</td>\n      <td>1149.232271</td>\n      <td>7.982567e+06</td>\n      <td>3.546049e-01</td>\n      <td>-9.350162e-01</td>\n      <td>4856.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>150572</th>\n      <td>9748.0</td>\n      <td>7321.0</td>\n      <td>12.685151</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>1.331512</td>\n      <td>9748.0</td>\n      <td>0.000103</td>\n      <td>0.000821</td>\n      <td>0.000137</td>\n      <td>...</td>\n      <td>0.000139</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.831231e+06</td>\n      <td>880.062845</td>\n      <td>6.342613e+06</td>\n      <td>1.000000e+00</td>\n      <td>-1.608123e-16</td>\n      <td>5259.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>150573</th>\n      <td>9729.0</td>\n      <td>7371.0</td>\n      <td>12.688347</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.319902</td>\n      <td>9729.0</td>\n      <td>0.000103</td>\n      <td>0.001028</td>\n      <td>0.000136</td>\n      <td>...</td>\n      <td>0.000140</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.935547e+06</td>\n      <td>2033.327409</td>\n      <td>1.453626e+07</td>\n      <td>8.854560e-01</td>\n      <td>4.647232e-01</td>\n      <td>3115.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>150574</th>\n      <td>9004.0</td>\n      <td>6856.0</td>\n      <td>12.590117</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.313302</td>\n      <td>9004.0</td>\n      <td>0.000111</td>\n      <td>0.000777</td>\n      <td>0.000146</td>\n      <td>...</td>\n      <td>0.000151</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6.657900e+04</td>\n      <td>101.640684</td>\n      <td>6.731662e+05</td>\n      <td>5.680647e-01</td>\n      <td>8.229839e-01</td>\n      <td>2253.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>150575</th>\n      <td>9886.0</td>\n      <td>7383.0</td>\n      <td>12.683961</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.339022</td>\n      <td>9886.0</td>\n      <td>0.000101</td>\n      <td>0.001012</td>\n      <td>0.000135</td>\n      <td>...</td>\n      <td>0.000138</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.176610e+06</td>\n      <td>1119.616163</td>\n      <td>8.097064e+06</td>\n      <td>8.854560e-01</td>\n      <td>-4.647232e-01</td>\n      <td>5573.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>150576 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "source": [
    "## Standardization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X = scaler.fit_transform(training)[:,:-1] # all except y\n",
    "train_Y = training[training.columns[-1]]\n",
    "\n",
    "test_X = scaler.transform(testing)[:,:-1] # all except y\n",
    "test_Y = testing[testing.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    81021\n",
       "3.0    22736\n",
       "4.0    22524\n",
       "1.0    12359\n",
       "2.0    11936\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "train_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    20207\n",
       "3.0     5745\n",
       "4.0     5605\n",
       "1.0     3112\n",
       "2.0     2975\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "test_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=train_X, columns=df.columns[:-1])"
   ]
  },
  {
   "source": [
    "## Two-stage approach\n",
    "### 1. Stage: Change Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_stage1_data(X, y: pd.Series) -> ('X', 'y'):\n",
    "    '''Simplify dataset classes to 0 -> 0, other -> 1.'''\n",
    "    y_stg1 = y.apply(lambda elem: 0 if elem == 0 else 1)\n",
    "    return X, y_stg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    81021\n",
       "1    69555\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "train_X_stg1, train_Y_stg1 = prepare_stage1_data(train_X, train_Y)\n",
    "train_Y_stg1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    20207\n",
       "1    17437\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "test_X_stg1, test_Y_stg1 = prepare_stage1_data(test_X, test_Y)\n",
    "test_Y_stg1.value_counts()"
   ]
  },
  {
   "source": [
    "### 2. Stage: Change Type Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "def prepare_stage2_data(X, y, columns) -> ('X', 'y'):\r\n",
    "    '''Remove class 0 from dataset.'''\r\n",
    "    xy = pd.DataFrame(data=X, columns=columns)\r\n",
    "    xy['evolution_label'] = y\r\n",
    "\r\n",
    "    # remove class 0\r\n",
    "    tmp = xy.loc[xy['evolution_label'] != 0.0].reset_index(drop=True)\r\n",
    "    X_stg2 = tmp[tmp.columns[:-1]]\r\n",
    "    Y_stg2 = tmp[tmp.columns[-1]]\r\n",
    "    \r\n",
    "    return X_stg2, Y_stg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.0    22736\n",
       "4.0    22524\n",
       "1.0    12359\n",
       "2.0    11936\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "train_X_stg2, train_Y_stg2 = prepare_stage2_data(train_X, train_Y, columns=training.columns[:-1])\n",
    "train_Y_stg2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.0    5745\n",
       "4.0    5605\n",
       "1.0    3112\n",
       "2.0    2975\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "test_X_stg2, test_Y_stg2 = prepare_stage2_data(test_X, test_Y, columns=testing.columns[:-1])\n",
    "test_Y_stg2.value_counts()"
   ]
  },
  {
   "source": [
    "## Balancing of Training Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    20207\n",
       "3.0     5745\n",
       "4.0     5605\n",
       "1.0     3112\n",
       "2.0     2975\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_Y_stg1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.0    10000\n",
       "4.0    10000\n",
       "2.0    10000\n",
       "1.0    10000\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "train_Y_stg2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import DataSampler\n",
    "sampler = DataSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    20000\n",
       "0    20000\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "# balancing by downsampling\n",
    "\n",
    "train_X_stg1, train_Y_stg1 = sampler.sample_fixed_size(train_X_stg1, train_Y_stg1, size=20000)\n",
    "train_Y_stg1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.0    10000\n",
       "4.0    10000\n",
       "2.0    10000\n",
       "1.0    10000\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "# balancing by downsampling \n",
    "\n",
    "train_X_stg2, train_Y_stg2 = sampler.sample_fixed_size(train_X_stg2, train_Y_stg2, size=10000)\n",
    "train_Y_stg2.value_counts()"
   ]
  },
  {
   "source": [
    "## Principal Components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "\n",
    "train_Xp = pca.fit_transform(train_X_stg2)\n",
    "test_Xp = pca.transform(test_X_stg2)"
   ]
  },
  {
   "source": [
    "## Evaluation Reports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def print_report(clfs: list, test_Xs: list, test_Y: 'y', titles: list):\n",
    "    \"\"\"\n",
    "    Prints all reports.\n",
    "    :param clfs: list of classifiers to evaluate\n",
    "    :param test_Xs: list of test_X for the corresponding classifier at idx\n",
    "    :param test_Y: true classes\n",
    "    :param titles: list of titles for the classifiers at idx\n",
    "    \"\"\"\n",
    "    for clf, test_X, title in zip(clfs, test_Xs, titles):\n",
    "        pred_Y = clf.predict(test_X)        \n",
    "        print(f\"### {title} ###\\n\", sklearn.metrics.classification_report(y_true=test_Y, y_pred=pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "def export_model(model, model_name):\n",
    "    return\n",
    "    \n",
    "    with open(f'data/{use_case}/ml_output/{approach}/{layer_name}_{model_name}.model', 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(clf1, clf2, test_X) -> 'pred_y':\n",
    "    '''Runs the two-stage approach by predicting first with clf1 then clf2.'''\n",
    "    # STG 1\n",
    "    pred_Y_stg1 = clf1.predict(test_X)   \n",
    "\n",
    "    # merge original X with predicted change Y1\n",
    "    test_xy = pd.DataFrame(data=test_X, columns=testing.columns[:-1])\n",
    "    test_xy['evolution_label'] = pred_Y_stg1\n",
    "    \n",
    "    # create new test set with from all predicted change=1\n",
    "    test_xy_stg2 = test_xy.loc[test_xy['evolution_label'] == 1.0]\n",
    "    test_X_stg2 = test_xy_stg2[test_xy_stg2.columns[:-1]]\n",
    "\n",
    "    # STG 2\n",
    "    pred_Y_stg2 = clf2.predict(test_X_stg2)\n",
    "\n",
    "    # merge stg2 X with predicted change type Y2\n",
    "    test_xy_stg2 = test_X_stg2\n",
    "    test_xy_stg2['evolution_label'] = pred_Y_stg2\n",
    "\n",
    "    # merge results based on original index (pred class 0 stays 0)\n",
    "    test_xy['evolution_label'].update(test_xy_stg2['evolution_label'])\n",
    "    pred_Y = test_xy['evolution_label']\n",
    "\n",
    "    return pred_Y"
   ]
  },
  {
   "source": [
    "# Naive Bayes\n",
    "Working best with _Xp_\n",
    "\n",
    "Parameters: \n",
    "- priors: _prior probabilities of classes_, none\n",
    "- var\\_smoothing: \\[_0_ , 1\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n               precision    recall  f1-score   support\n\n         1.0       0.39      0.15      0.22      3112\n         2.0       0.25      0.82      0.38      2975\n         3.0       0.49      0.34      0.40      5745\n         4.0       0.50      0.23      0.31      5605\n\n    accuracy                           0.35     17437\n   macro avg       0.41      0.38      0.33     17437\nweighted avg       0.43      0.35      0.34     17437\n\n### Xp ###\n               precision    recall  f1-score   support\n\n         1.0       0.33      0.72      0.45      3112\n         2.0       0.30      0.28      0.29      2975\n         3.0       0.49      0.49      0.49      5745\n         4.0       0.51      0.19      0.28      5605\n\n    accuracy                           0.40     17437\n   macro avg       0.41      0.42      0.38     17437\nweighted avg       0.43      0.40      0.38     17437\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "priors = None #np.array([8,2,2,1,1]) / (8+2+2+1+1)\n",
    "smoothing = 1E-9\n",
    "\n",
    "clf = GaussianNB(priors=priors, var_smoothing=smoothing)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "clf_p = GaussianNB(priors=priors, var_smoothing=smoothing)\n",
    "clf_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([clf, clf_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(clf, 'nb_x')\n",
    "export_model(clf_p, 'nb_xp')"
   ]
  },
  {
   "source": [
    "# Support Vector Machine\n",
    "Parameters:\n",
    "- kernel: _linear_, rbf, poly, sigmoid\n",
    "- C (regularization): <1, _1_, >1\n",
    "- class\\_weight: _None_, balanced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-356c6809915a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msvc_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/community-prediction/venv-gpu2/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/community-prediction/venv-gpu2/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "c = 1\n",
    "kernel = 'linear'\n",
    "\n",
    "svc = SVC(kernel='linear', C=c)\n",
    "svc.fit(train_X, train_Y)\n",
    "\n",
    "svc_p = SVC(kernel='linear', C=c)\n",
    "svc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([svc, svc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(svc, 'svc_x')\n",
    "export_model(svc_p, 'svc_xp')"
   ]
  },
  {
   "source": [
    "# K-nearest Neighbors\n",
    "Parameters:\n",
    "- n\\_neighbors: _30_\n",
    "- weights: _uniform_, distance\n",
    "- algorithm: _auto_, ball_tree, kd_tree, brute\n",
    "- leaf\\_size: _50_ (no difference)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(leaf_size=50, n_neighbors=30)"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_neighbors = 30\n",
    "weights = 'uniform'\n",
    "algo = 'auto'\n",
    "leaf_size = 50\n",
    "\n",
    "knnc1 = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algo, leaf_size=leaf_size)\n",
    "knnc1.fit(train_X_stg1, train_Y_stg1)\n",
    "\n",
    "knnc2 = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights,  algorithm=algo, leaf_size=leaf_size)\n",
    "knnc2.fit(train_X_stg2, train_Y_stg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = predict_all(knnc1, knnc2, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n         0.0       0.70      0.79      0.74     20207\n         1.0       0.38      0.48      0.42      3112\n         2.0       0.35      0.42      0.38      2975\n         3.0       0.26      0.16      0.20      5745\n         4.0       0.23      0.15      0.18      5605\n\n    accuracy                           0.54     37644\n   macro avg       0.38      0.40      0.38     37644\nweighted avg       0.51      0.54      0.52     37644\n\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_true=test_Y, y_pred=pred_Y))"
   ]
  },
  {
   "source": [
    "# Decision Tree\n",
    "Working well with _Xp_\n",
    "\n",
    "Parameters:\n",
    "- criterion: _gini_, entropy\n",
    "- splitter: best, _random_\n",
    "- max_depth: default=None\n",
    "- min_samples_leaf (to construct leaf): default=1\n",
    "- min_impurity_decrease (split if the impurity is then decreased by): default=0\n",
    "- ccp_alpha (max allowed cost after pruning): default=0/nopruning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n               precision    recall  f1-score   support\n\n         1.0       0.37      0.52      0.43      3112\n         2.0       0.35      0.38      0.37      2975\n         3.0       0.48      0.48      0.48      5745\n         4.0       0.48      0.35      0.40      5605\n\n    accuracy                           0.43     17437\n   macro avg       0.42      0.43      0.42     17437\nweighted avg       0.44      0.43      0.43     17437\n\n### Xp ###\n               precision    recall  f1-score   support\n\n         1.0       0.33      0.54      0.41      3112\n         2.0       0.29      0.35      0.32      2975\n         3.0       0.47      0.44      0.45      5745\n         4.0       0.48      0.29      0.36      5605\n\n    accuracy                           0.39     17437\n   macro avg       0.39      0.41      0.39     17437\nweighted avg       0.42      0.39      0.39     17437\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "criterion = 'entropy'\n",
    "splitter = 'random'\n",
    "max_depth = None\n",
    "min_samples_leaf = 2\n",
    "min_impurity_decrease = 0 # impurity improvement needed to split\n",
    "ccp_alpha = 0\n",
    "\n",
    "seed=42\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, ccp_alpha=ccp_alpha, random_state=seed)\n",
    "dtc.fit(train_X, train_Y)\n",
    "\n",
    "dtc_p = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, ccp_alpha=ccp_alpha, random_state=seed)\n",
    "dtc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([dtc, dtc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(dtc, 'dt_x')\n",
    "export_model(dtc_p, 'dt_xp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n               precision    recall  f1-score   support\n\n         0.0       0.69      0.37      0.48     20207\n         1.0       0.28      0.41      0.33      3112\n         2.0       0.26      0.39      0.31      2975\n         3.0       0.19      0.30      0.23      5745\n         4.0       0.18      0.28      0.22      5605\n\n    accuracy                           0.35     37644\n   macro avg       0.32      0.35      0.32     37644\nweighted avg       0.47      0.35      0.38     37644\n\n### Xp ###\n               precision    recall  f1-score   support\n\n         0.0       0.69      0.37      0.48     20207\n         1.0       0.28      0.40      0.33      3112\n         2.0       0.26      0.39      0.31      2975\n         3.0       0.19      0.30      0.23      5745\n         4.0       0.18      0.29      0.22      5605\n\n    accuracy                           0.35     37644\n   macro avg       0.32      0.35      0.32     37644\nweighted avg       0.47      0.35      0.38     37644\n\n"
     ]
    }
   ],
   "source": [
    "print_report([dtc, dtc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])"
   ]
  },
  {
   "source": [
    "# Random Forest\n",
    "Parameters:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.41      0.45      0.43      3112\n",
      "         2.0       0.38      0.43      0.41      2975\n",
      "         3.0       0.48      0.45      0.46      5745\n",
      "         4.0       0.47      0.45      0.46      5605\n",
      "\n",
      "    accuracy                           0.45     17437\n",
      "   macro avg       0.44      0.44      0.44     17437\n",
      "weighted avg       0.45      0.45      0.45     17437\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.41      0.44      0.42      3112\n",
      "         2.0       0.38      0.43      0.40      2975\n",
      "         3.0       0.47      0.45      0.46      5745\n",
      "         4.0       0.47      0.45      0.46      5605\n",
      "\n",
      "    accuracy                           0.44     17437\n",
      "   macro avg       0.43      0.44      0.44     17437\n",
      "weighted avg       0.45      0.44      0.44     17437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = 100\n",
    "criterion = 'entropy'\n",
    "max_depth = None\n",
    "min_samples_leaf = 2\n",
    "min_impurity_decrease= 0\n",
    "bootstrap=True\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, bootstrap=bootstrap)\n",
    "rfc.fit(train_X, train_Y)\n",
    "\n",
    "rfc_p = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, bootstrap=bootstrap)\n",
    "rfc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([rfc, rfc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(rfc, 'rf_x')\n",
    "export_model(rfc_p, 'rf_xp')"
   ]
  },
  {
   "source": [
    "# Boosting\n",
    "Parameters:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "base_estimator= SVC(kernel='linear')\n",
    "n_estimators= 100\n",
    "algo = 'SAMME'\n",
    "learning_rate = .3\n",
    "\n",
    "bc = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=n_estimators, algorithm=algo, learning_rate=learning_rate)\n",
    "bc.fit(train_X, train_Y)\n",
    "\n",
    "bc_p = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=n_estimators, algorithm=algo, learning_rate=learning_rate)\n",
    "bc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([bc, bc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(bc, 'boost_x')\n",
    "export_model(bc_p, 'boost_xp')"
   ]
  },
  {
   "source": [
    "# Pipeline Approac"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_neighbors = 30\n",
    "weights = 'uniform'\n",
    "algo = 'auto'\n",
    "leaf_size = 50\n",
    "\n",
    "knnc = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algo, leaf_size=leaf_size)\n",
    "knnc.fit(train_X, train_Y)\n",
    "\n",
    "\n",
    "knnc2 ="
   ]
  }
 ]
}