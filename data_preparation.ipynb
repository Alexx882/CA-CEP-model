{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0edc7e8da127e731d54753afe8930f1420dc6ae9a13010eedc53dff7bbda352d4",
   "display_name": "Python 3.6.9  ('venv-gpu2': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "edc7e8da127e731d54753afe8930f1420dc6ae9a13010eedc53dff7bbda352d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Contextualization\n",
    "Raw transactional data is loaded and columns of interest are identified for contextualization (layering)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clustering\n",
    "Each layer is clustered independently over all time windows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Temporal Community Segmentation\n",
    "Clusters are split up based on their timestamp into multiple time windows\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Feature Engineering\n",
    "_Features are extracted for each cluster:_\n",
    "- cluster size\n",
    "- cluster standard deviation\n",
    "- cluster scarcity\n",
    "- cluster popularity (importance I)\n",
    "- cluster diversity (importance II)\n",
    "- cluster area\n",
    "- cluster temporal center distance\n",
    "\n",
    "\n",
    "_Features are extracted for each layer:_\n",
    "- number of nodes\n",
    "- number of clusters\n",
    "- layer entropy\n",
    "- relative cluster sizes\n",
    "- center of clusters\n",
    "- distance from global centers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'youtube'\n",
    "\n",
    "layers = [\n",
    "    ['CategoryLayer', 'category_id'],\n",
    "    ['ViewsLayer', 'views'],\n",
    "    ['LikesLayer', 'likes'],\n",
    "    ['DislikesLayer', 'dislikes'],\n",
    "    ['CommentCountLayer', 'comment_count'],\n",
    "    ['CountryLayer', 'country_id'],\n",
    "    ['TrendDelayLayer', 'trend_delay'],\n",
    "]"
   ]
  },
  {
   "source": [
    "## Cluster Metrics Calculation\n",
    "Calculating raw metrics:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from processing import ClusterMetricsCalculatorFactory\n",
    "\n",
    "def calculate_center(cluster: dict, features: list) -> Tuple[float]:\n",
    "    calc = ClusterMetricsCalculatorFactory.create_metrics_calculator(cluster['nodes'], features, 1, 1)\n",
    "    return calc.get_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "from entities import TimeWindow, Cluster\n",
    "\n",
    "def store_metrics_for_clusters(layer_name: str, feature_names: List[str]):\n",
    "    '''\n",
    "    :param layer_name: Name of the layer for which multiple time windows exist\n",
    "    :param feature_names: Features of the layer\n",
    "    '''\n",
    "    print(f\"Working on {layer_name} cluster metrics\")\n",
    "\n",
    "    # load global cluster centers\n",
    "    path_in = f'data/{dataset}/raw/clusters/{layer_name}.json'\n",
    "    with open(path_in, 'r') as file:\n",
    "        clusters = json.loads(file.read())\n",
    "        cluster_centers: Dict[str, Tuple[float]] = { \n",
    "            str(cluster['cluster_label']): calculate_center(cluster, feature_names) \n",
    "            for cluster in clusters \n",
    "            if cluster['label'] != 'noise' \n",
    "            }\n",
    "\n",
    "    path_in = f'data/{dataset}/raw/timeslices/{layer_name}'\n",
    "    path_out = f'data/{dataset}/cluster_metrics/{layer_name}.json'\n",
    "\n",
    "    complete_clusters: List[Cluster] = []\n",
    "\n",
    "    for root, _, files in os.walk(path_in):\n",
    "        for f in files:\n",
    "            with open(os.path.join(root, f), 'r') as file:\n",
    "                # for each time window json\n",
    "                json_slice = json.loads(file.read())\n",
    "                time_window = TimeWindow.create_from_serializable_dict(json_slice)\n",
    "\n",
    "                # create all clusters + metrics for one time window\n",
    "                clusters = Cluster.create_multiple_from_time_window(time_window, feature_names, cluster_centers)\n",
    "                complete_clusters.extend(clusters)\n",
    "        \n",
    "    # store the cluster metrics\n",
    "    with open(path_out, 'w') as file:\n",
    "        file.write(json.dumps([cl.__dict__ for cl in complete_clusters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    store_metrics_for_clusters(layer[0], layer[1])"
   ]
  },
  {
   "source": [
    "Preparing ML input with N=3 cluster metrics data (X) with cluster evolution label (y):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['cluster_size', 'cluster_variance', 'cluster_density', 'cluster_import1', 'cluster_import2', \n",
    "        'cluster_area', 'cluster_center_distance', 'time_f1', 'time_f2']*3 + ['evolution_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from entities import Cluster\n",
    "import collections\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "def get_evolution_label(old_size: int, new_size: int) -> int:\n",
    "    '''Returns the evolution label as int by mapping 0..4 to {continuing, shrinking, growing, dissolving, forming}.'''\n",
    "    if old_size == 0 and new_size == 0:\n",
    "        return -1 # STILL EMPTY\n",
    "    if old_size == new_size:\n",
    "        return 0 # continuing\n",
    "    if old_size == 0 and new_size > 0:\n",
    "        return 4 # forming\n",
    "    if old_size > 0 and new_size == 0:\n",
    "        return 3 # dissolving\n",
    "    if old_size > new_size:\n",
    "        return 1 # shrinking\n",
    "    if old_size < new_size:\n",
    "        return 2 # growing\n",
    "\n",
    "def get_cyclic_time_feature(time: int, max_time_value: int = 52) -> (float, float):\n",
    "    return (np.sin(2*np.pi*time/max_time_value),\n",
    "            np.cos(2*np.pi*time/max_time_value))\n",
    "\n",
    "def create_metrics_training_data(layer_name: str, N: int = 3) -> Iterable[list]:\n",
    "    \"\"\"\n",
    "    Loads the metrics training data for an individual layer from disk.\n",
    "    A single metrics training data point should look like this:\n",
    "\n",
    "    (cluster_size, cluster_std_dev, cluster_scarcity, cluster_import1, cluster_import2, cluster_range, cluster_center_x, cluster_center_y, time_info) ^ N, evolution_label\n",
    "    time_info ... the time as 2d cyclic feature, i.e. time_info := (time_f1, time_f2)\n",
    "\n",
    "    The first tuple represents metrics from the cluster in t_i-(N-1).\n",
    "    The Nth tuple represents metrics from the cluster in t_i.\n",
    "    The label is one of {continuing, shrinking, growing, dissolving, forming} \\ {splitting, merging} and identifies the change for t_i+1.\n",
    "    \n",
    "    :param N: number of cluster metric tuples\n",
    "    :param layer_name: the name of the layer metrics json file\n",
    "    \"\"\"\n",
    "    \n",
    "    path_in = f\"data/{dataset}/cluster_metrics/{layer_name}.json\"\n",
    "    with open(path_in, 'r') as file:\n",
    "        data = [Cluster.create_from_dict(cl_d) for cl_d in json.loads(file.read())]\n",
    "\n",
    "    data.sort(key=lambda cl: (cl.cluster_id, cl.time_window_id))\n",
    "\n",
    "    # manually prepare deque with N metric_tuples + evolution label\n",
    "    tuples = []\n",
    "\n",
    "    for i, cur_cluster in enumerate(data[:-1]):\n",
    "\n",
    "        if cur_cluster.cluster_id != data[i+1].cluster_id:\n",
    "            # next cluster slice in list will be another cluster id -> restart deque and skip adding the current (last) cluster slice\n",
    "            tuples = []\n",
    "            continue\n",
    "\n",
    "        cur_metrics = (cur_cluster.size, cur_cluster.std_dev, cur_cluster.scarcity, cur_cluster.importance1, cur_cluster.importance2, cur_cluster.range_, cur_cluster.global_center_distance, get_cyclic_time_feature(cur_cluster.get_time_info()))\n",
    "\n",
    "        # deque function: adding N+1st element will remove oldest one\n",
    "        if len(tuples) == N:\n",
    "            tuples.pop(0)\n",
    "        tuples.append(cur_metrics)\n",
    "\n",
    "        if len(tuples) == N:\n",
    "            label = get_evolution_label(cur_cluster.size, data[i+1].size)\n",
    "            yield list(tuples) + [label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_metrics_datapoint(datapoint: list) -> ('X, y: np.array'):\n",
    "    '''\n",
    "    Flattens a single metrics data point in the form:\n",
    "    [(cluster_size, cluster_variance, cluster_density, cluster_import1, cluster_import2, cluster_range, cluster_center, (time_f1, time_f2))^N, evolution_label]\n",
    "    to:\n",
    "    (X, y: np.array\n",
    "    '''\n",
    "    flat_list = []\n",
    "    for entry in datapoint[:-1]: # for all x\n",
    "        flat_list.extend(entry[:-1]) # add all number features except the time tuple\n",
    "        flat_list.extend(entry[-1]) # add time tuple\n",
    "\n",
    "    flat_list.append(datapoint[-1]) # y\n",
    "    return np.asarray(flat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_metrics_data_to_dataframe(data: Iterable, columns: list, flattening_method: 'callable') -> pd.DataFrame:\n",
    "    '''Flattens and splits metrics data to match ML conventions.'''\n",
    "    training_data = []\n",
    "\n",
    "    for element in data:\n",
    "        xy: 'np.array' = flattening_method(element)\n",
    "        \n",
    "        training_data.append(xy)\n",
    "\n",
    "    return pd.DataFrame(data=training_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import collections\n",
    "import statistics as stat\n",
    "\n",
    "def balance_dataset(df: DataFrame) -> DataFrame:\n",
    "    # TODO\n",
    "    return df\n",
    "\n",
    "def store_training_data(layer_name: str):\n",
    "    # load metrics data from disk\n",
    "    data: Iterable = create_metrics_training_data(layer_name=layer_name)\n",
    "    \n",
    "    # flatten and convert to df\n",
    "    df = convert_metrics_data_to_dataframe(data, columns=COLUMNS, flattening_method=flatten_metrics_datapoint)\n",
    "\n",
    "    # balance df\n",
    "    df = balance_dataset(df)\n",
    "\n",
    "    # shuffle\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    df.to_csv(f'data/{dataset}/ml_input/single_context/{layer_name}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, _ in layers:\n",
    "    print(f\"Creating training data for {name}\")\n",
    "    store_training_data(layer_name=name)"
   ]
  },
  {
   "source": [
    "## Layer Metrics Calculation\n",
    "Calculating raw metrics:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import statistics as stat\n",
    "import json\n",
    "import os\n",
    "from entities import TimeWindow, Layer\n",
    "from processing import ClusterMetricsCalculatorFactory\n",
    "\n",
    "def store_metrics_for_layers(layer_name: str = 'CallTypeLayer', feature_names: List[str] = ['call_type']):\n",
    "    print(f\"Working on {layer_name} layer metrics\")\n",
    "\n",
    "    # load global cluster centers\n",
    "    path_in = f'data/{dataset}/raw/clusters/{layer_name}.json'\n",
    "    with open(path_in, 'r') as file:\n",
    "        clusters = json.loads(file.read())\n",
    "        cluster_centers: Dict[str, Tuple[float]] = { \n",
    "            str(cluster['cluster_label']): calculate_center(cluster, feature_names) \n",
    "            for cluster in clusters \n",
    "            if cluster['label'] != 'noise' \n",
    "            }\n",
    "\n",
    "    # load time windows \n",
    "    all_layers: List[Layer] = []\n",
    "    path_in = f'data/{dataset}/raw/timeslices/{layer_name}'\n",
    "    for root, _, files in os.walk(path_in):\n",
    "        for f in files:\n",
    "            with open(os.path.join(root, f), 'r') as file:\n",
    "                json_time_slice = json.loads(file.read())\n",
    "                time_window = TimeWindow.create_from_serializable_dict(json_time_slice)\n",
    "\n",
    "                layer = Layer.create_from_time_window(time_window, feature_names, cluster_centers)\n",
    "                all_layers.append(layer)\n",
    "        \n",
    "    # store the layer metrics\n",
    "    path_out = f'data/{dataset}/layer_metrics/{layer_name}.json'\n",
    "    with open(path_out, 'w') as file:\n",
    "        file.write(json.dumps([l.__dict__ for l in all_layers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    try:\n",
    "        store_metrics_for_layers(layer[0], layer[1])\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "source": [
    "Preparing ML input with N=2 layer metrics data (X) with cluster evolution label (y):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_columns(N) -> List[str]:\n",
    "    '''Returns columns for the data depending on sizes of N (number time windows) independent of M (number of clusters in L_R).'''\n",
    "\n",
    "    cols = ['n_nodes', 'n_clusters', 'entropy']\n",
    "\n",
    "    for v in ['sizes', 'relative_sizes', 'center_dist']:\n",
    "        cols += [f'{v}_min', f'{v}_max', f'{v}_avg', f'{v}_sum']\n",
    "\n",
    "    # cols.extend(['relative_cluster_sizes']*M)\n",
    "    # cols.extend(['cluster_centers']*M)\n",
    "    # cols.extend(['distance_from_global_centers']*M)\n",
    "\n",
    "    cols.extend(['time_f1', 'time_f2'])\n",
    "\n",
    "    cols = cols * N \n",
    "    return cols + ['cluster_id'] + ['evolution_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cyclic_time_feature_from_time_window(time: str) -> (float, float):\n",
    "    return get_cyclic_time_feature(int(time.replace('(', '').replace(')', '').split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Dict, Any\n",
    "import json\n",
    "from entities import Layer, Cluster\n",
    "\n",
    "def get_layer_metrics(layer: Layer) -> Iterable:\n",
    "    res = [layer.n_nodes, layer.n_clusters, layer.entropy]\n",
    "    res += [layer.cluster_size_agg_metrics[k] for k in ['min', 'max', 'avg', 'sum']]\n",
    "    res += [layer.cluster_relative_size_agg_metrics[k] for k in ['min', 'max', 'avg', 'sum']]\n",
    "    res += [layer.cluster_center_distance_agg_metrics[k] for k in ['min', 'max', 'avg', 'sum']]\n",
    "    res.append(get_cyclic_time_feature_from_time_window(layer.time_window_id))\n",
    "    return res\n",
    "\n",
    "def create_layer_metrics_training_data(layer_name: str, reference_layer: str, N: int = 2) -> Iterable:\n",
    "    \"\"\"\n",
    "    Loads the metrics training data for an individual layer from disk.\n",
    "    \n",
    "    A single metrics training data point should look like this:\n",
    "\n",
    "    [(n_nodes, n_clusters, entropy,\n",
    "     (relative_cluster_size)^M, (cluster_centers)^M, (distance_from_global_centers)^M, \n",
    "     (time1, time2))^N, \n",
    "     cluster_number, evolution_label]\n",
    "     \n",
    "    The first tuple represents metrics from the reference layer in t_i-(N-1).\n",
    "    The Nth tuple represents metrics from the reference layer in t_i.\n",
    "    The reference_layer has M clusters in total, this might differ from the number of clusters in layer_name.\n",
    "    The cluster number identifies the cluster for which the evolution_label holds. \n",
    "    The label is one of {continuing, shrinking, growing, dissolving, forming} \\ {splitting, merging} and identifies the change for a cluster in the layer layer_name for t_i.\n",
    "    \n",
    "    # TODO N is not implemented and fixed to 2\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f'data/{dataset}/cluster_metrics/{layer_name}.json') as file:\n",
    "        cluster_metrics: List[Cluster] = [Cluster.create_from_dict(e) for e in json.loads(file.read())]\n",
    "        cluster_ids = {c.cluster_id for c in cluster_metrics}\n",
    "        cluster_metrics: Dict[Any, Cluster] = {(c.time_window_id, c.cluster_id): c for c in cluster_metrics}\n",
    "        \n",
    "    with open(f'data/{dataset}/layer_metrics/{reference_layer}.json') as file:\n",
    "        layer_metrics: List[Layer] = [Layer.create_from_dict(e) for e in json.loads(file.read())]\n",
    "        layer_metrics: Dict[Any, Layer] = {l.time_window_id: l for l in layer_metrics}\n",
    "\n",
    "    # load the time keys chronologically\n",
    "    ordered_time_keys = list(layer_metrics.keys())\n",
    "    ordered_time_keys.sort(key=lambda x: [int(v) for v in x.replace('(', '').replace(')', '').split(',')])\n",
    "    \n",
    "    # go through all time windows once...\n",
    "    prev_time_key = ordered_time_keys[0]\n",
    "    for current_time_key in ordered_time_keys[1:]:\n",
    "        # ...and load the current and previous layer metrics in the reference_layer\n",
    "        current_layer_metric = layer_metrics[current_time_key]\n",
    "        prev_layer_metric = layer_metrics[prev_time_key]\n",
    "\n",
    "        current_layer_metric_tuple = get_layer_metrics(current_layer_metric)\n",
    "        prev_layer_metric_tuple = get_layer_metrics(prev_layer_metric)\n",
    "\n",
    "        # ...then load the current and previous cluster metrics for all clusters in the layer_name\n",
    "        for cluster_id in cluster_ids:\n",
    "            current_cluster_metric = cluster_metrics[(current_time_key, cluster_id)]\n",
    "            prev_cluster_metric = cluster_metrics[(prev_time_key, cluster_id)]\n",
    "            evolution_label = get_evolution_label(prev_cluster_metric.size, current_cluster_metric.size)\n",
    "\n",
    "            # yield each combination of reference layer metrics to clusters\n",
    "            yield [prev_layer_metric_tuple, current_layer_metric_tuple, int(cluster_id), evolution_label]\n",
    "\n",
    "        prev_time_key = current_time_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer_metrics_datapoint(datapoint: list) -> ('X, y: np.array'):\n",
    "    '''\n",
    "    Flattens a single layer metrics data point in the form:\n",
    "    [(n_nodes, n_clusters, entropy,\n",
    "     (relative_cluster_size)^M, (distance_from_global_centers)^M, \n",
    "     (time1, time2))^N, \n",
    "     cluster_number, evolution_label]\n",
    "    to:\n",
    "    (X, y: np.array)\n",
    "    '''\n",
    "    flat_list = []\n",
    "    for layer_metric_tuple in datapoint[:-2]: # for all x\n",
    "        flat_list.extend(layer_metric_tuple[0:-1]) # everything before time\n",
    "        flat_list.extend(layer_metric_tuple[-1]) # time1/2\n",
    "\n",
    "    flat_list.append(datapoint[-2]) # cluster num\n",
    "    flat_list.append(datapoint[-1]) # y\n",
    "\n",
    "    return np.asarray(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import collections\n",
    "import statistics as stat\n",
    "\n",
    "def balance_dataset(df: DataFrame) -> ('X: np.array', 'Y: np.array'):\n",
    "    '''Balances an unbalanced dataset by ignoring elements from the majority label, so that majority-label data size = median of other cluster sizes.'''\n",
    "    # TODO\n",
    "    return df\n",
    "\n",
    "def store_training_data(layer_name: str, reference_layer_name: str):\n",
    "    # load metrics data from disk\n",
    "    data: Iterable = create_layer_metrics_training_data(layer_name=layer_name, reference_layer=reference_layer_name)\n",
    "    \n",
    "    # convert to X and Y\n",
    "    df = convert_metrics_data_to_dataframe(data, columns=get_columns(N=2), flattening_method=flatten_layer_metrics_datapoint)\n",
    "    \n",
    "    # balance df\n",
    "    df = balance_dataset(df)\n",
    "    \n",
    "    # shuffle\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(f'data/{dataset}/ml_input/cross_context/{layer_name}_{reference_layer_name}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dependencies = [\n",
    "    ('CategoryLayer', 'CountryLayer'),\n",
    "    ('ViewsLayer', 'CountryLayer'),\n",
    "\n",
    "    ('ViewsLayer', 'CategoryLayer'),\n",
    "\n",
    "    ('LikesLayer', 'ViewsLayer'),\n",
    "    ('DislikesLayer', 'ViewsLayer'),\n",
    "    ('CommentCountLayer', 'ViewsLayer'),\n",
    "    ('TrendDelayLayer', 'ViewsLayer'),\n",
    "]\n",
    "\n",
    "for l, l_r in layer_dependencies:\n",
    "    print(f\"Creating training data for {l} with L_R={l_r}\")\n",
    "    store_training_data(layer_name=l, reference_layer_name=l_r)"
   ]
  }
 ]
}