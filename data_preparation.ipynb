{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0ac79ad19892b6e891e8d97ca5fdbb2e2457e6e4ba8b10fb20aa9e37280e031f3",
   "display_name": "Python 3.7.8  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac79ad19892b6e891e8d97ca5fdbb2e2457e6e4ba8b10fb20aa9e37280e031f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Contextualization\n",
    "Raw transactional data is loaded and columns of interest are identified for contextualization (layering)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Clustering\n",
    "Each layer is clustered independently over all time windows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Temporal Community Segmentation\n",
    "Clusters are split up based on their timestamp into multiple time windows\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Feature Engineering\n",
    "_Features are extracted for each cluster:_\n",
    "- cluster size\n",
    "- cluster standard deviation\n",
    "- cluster scarcity\n",
    "- cluster popularity (importance I)\n",
    "- cluster diversity (importance II)\n",
    "- cluster range/needed space\n",
    "- cluster center\n",
    "\n",
    "\n",
    "_Features are extracted for each layer:_\n",
    "- relative cluster sizes\n",
    "- layer entropy\n",
    "- distance from global centers\n",
    "\n",
    "new:\n",
    "- number of nodes\n",
    "- number of clusters\n",
    "- center of clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Cluster Metrics Calculation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "from entities import TimeWindow, Cluster\n",
    "\n",
    "def calculate_metrics_for_clusters(layer_name: str, feature_names: List[str]):\n",
    "    '''\n",
    "    :param layer_name: Name of the layer for which multiple time windows exist\n",
    "    :param feature_names: Features of the layer\n",
    "    '''\n",
    "    print(f\"Working on {layer_name}\")\n",
    "\n",
    "    path_in = f'input/timeslices/{layer_name}'\n",
    "    path_out = f'input/metrics/{layer_name}.json'\n",
    "\n",
    "    complete_clusters: List[Cluster] = []\n",
    "\n",
    "    for root, _, files in os.walk(path_in):\n",
    "        for f in files:\n",
    "            with open(os.path.join(root, f), 'r') as file:\n",
    "                # for each time window json\n",
    "                json_slice = json.loads(file.read())\n",
    "                time_window = TimeWindow.create_from_serializable_dict(json_slice)\n",
    "\n",
    "                # create all clusters + metrics for one time window\n",
    "                clusters = Cluster.create_multiple_from_time_window(time_window, feature_names)\n",
    "                complete_clusters.extend(clusters)\n",
    "        \n",
    "    # store the cluster metrics\n",
    "    with open(path_out, 'w') as file:\n",
    "        file.write(json.dumps([cl.__dict__ for cl in complete_clusters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ['CallTypeLayer', 'call_type'],\n",
    "    ['DayTypeLayer', 'day_type'],\n",
    "    ['TaxiIdLayer', 'taxi_id'],\n",
    "\n",
    "    ['OriginCallLayer', ('call_type', 'origin_call')],\n",
    "    ['OriginStandLayer', ('call_type', 'origin_stand')],\n",
    "    ['StartLocationLayer', ('start_location_lat', 'start_location_long')],\n",
    "    ['EndLocationLayer', ('end_location_lat', 'end_location_long')],\n",
    "]\n",
    "\n",
    "for layer in layers:\n",
    "    calculate_metrics_for_clusters(layer[0], layer[1])"
   ]
  },
  {
   "source": [
    "## ML Input Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from entities import Cluster\n",
    "import collections\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "def get_evolution_label(old_size: int, new_size: int) -> int:\n",
    "    '''Returns the evolution label as int by mapping 0..4 to {continuing, shrinking, growing, dissolving, forming}.'''\n",
    "    if old_size == new_size:\n",
    "        return 0 # continuing\n",
    "    if old_size == 0 and new_size > 0:\n",
    "        return 4 # forming\n",
    "    if old_size > 0 and new_size == 0:\n",
    "        return 3 # dissolving\n",
    "    if old_size > new_size:\n",
    "        return 1 # shrinking\n",
    "    if old_size < new_size:\n",
    "        return 2 # growing\n",
    "\n",
    "def get_cyclic_time_feature(time: int, max_time_value: int = 52) -> (float, float):\n",
    "    return (np.sin(2*np.pi*time/max_time_value),\n",
    "            np.cos(2*np.pi*time/max_time_value))\n",
    "\n",
    "def create_metrics_training_data(layer_name: str, N: int = 3) -> Iterable[list]:\n",
    "    \"\"\"\n",
    "    Loads the metrics training data for an individual layer from disk.\n",
    "    A single metrics training data point should look like this:\n",
    "\n",
    "    (cluster_size, cluster_std_dev, cluster_scarcity, cluster_import1, cluster_import2, cluster_range, cluster_center, time_info) ^ N, evolution_label\n",
    "    time_info ... the time as 2d cyclic feature, i.e. time_info := (time_f1, time_f2)\n",
    "\n",
    "    The first tuple represents metrics from the cluster in t_i-(N-1).\n",
    "    The Nth tuple represents metrics from the cluster in t_i.\n",
    "    The label is one of {continuing, shrinking, growing, dissolving, forming} \\ {splitting, merging} and identifies the change for t_i+1.\n",
    "    \n",
    "    :param N: number of cluster metric tuples\n",
    "    :param layer_name: the name of the layer metrics json file\n",
    "    \"\"\"\n",
    "    \n",
    "    path_in = f\"input/metrics/{layer_name}.json\"\n",
    "    with open(path_in, 'r') as file:\n",
    "        data = [Cluster.create_from_dict(cl_d) for cl_d in json.loads(file.read())]\n",
    "\n",
    "    data.sort(key=lambda cl: (cl.cluster_id, cl.time_window_id))\n",
    "\n",
    "    # manually prepare deque with N metric_tuples + evolution label\n",
    "    tuples = []\n",
    "\n",
    "    for i, cur_cluster in enumerate(data[:-1]):\n",
    "\n",
    "        if cur_cluster.cluster_id != data[i+1].cluster_id:\n",
    "            # next cluster slice in list will be another cluster id -> restart deque and skip adding the current (last) cluster slice\n",
    "            tuples = []\n",
    "            continue\n",
    "\n",
    "        cur_metrics = (cur_cluster.size, cur_cluster.std_dev, cur_cluster.scarcity, cur_cluster.importance1, cur_cluster.importance2, cur_cluster.range_, cur_cluster.center, get_cyclic_time_feature(cur_cluster.get_time_info()))\n",
    "\n",
    "        # deque function: adding N+1st element will remove oldest one\n",
    "        if len(tuples) == N:\n",
    "            tuples.pop(0)\n",
    "        tuples.append(cur_metrics)\n",
    "\n",
    "        if len(tuples) == N:\n",
    "            label = get_evolution_label(cur_cluster.size, data[i+1].size)\n",
    "            yield list(tuples) + [label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_metrics_datapoint(datapoint: list) -> ('X, y: np.array'):\n",
    "    '''\n",
    "    Flattens a single metrics data point in the form:\n",
    "    [(cluster_size, cluster_variance, cluster_density, cluster_import1, cluster_import2, cluster_range, cluster_center, (time_f1, time_f2))^N, evolution_label]\n",
    "    to:\n",
    "    (X, y: np.array\n",
    "    '''\n",
    "    flat_list = []\n",
    "    for entry in datapoint[:-1]: # for all x\n",
    "        flat_list.extend(entry[:-1]) # add all number features except the time tuple\n",
    "        flat_list.extend(entry[-1]) # add time tuple\n",
    "\n",
    "    flat_list.append(datapoint[-1]) # y\n",
    "    return np.asarray(flat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_metrics_data_to_dataframe(data: Iterable, columns=['cluster_size', 'cluster_variance', 'cluster_density', 'cluster_import1', 'cluster_import2', \n",
    "        'cluster_range', 'cluster_center', 'time_f1', 'time_f2']*3 + ['evolution_label']) -> pd.DataFrame:\n",
    "    '''Flattens and splits metrics data to match ML conventions.'''\n",
    "    training_data = []\n",
    "\n",
    "    for element in data:\n",
    "        xy: 'np.array' = flatten_metrics_datapoint(element)\n",
    "        \n",
    "        training_data.append(xy)\n",
    "\n",
    "    return pd.DataFrame(data=training_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   cluster_size  cluster_variance  cluster_density  cluster_import1  \\\n",
       "0             1                 1                1                1   \n",
       "1             1                 1                1                7   \n",
       "\n",
       "   cluster_import2  cluster_range  cluster_center  time_f1  time_f2  \\\n",
       "0                1              1               1        1        1   \n",
       "1                1              1               1        1        1   \n",
       "\n",
       "   cluster_size  ...  cluster_size  cluster_variance  cluster_density  \\\n",
       "0             2  ...             3                 3                3   \n",
       "1             2  ...             3                 3                3   \n",
       "\n",
       "   cluster_import1  cluster_import2  cluster_range  cluster_center  time_f1  \\\n",
       "0                3                3              3               3        3   \n",
       "1                3                3              7               3        3   \n",
       "\n",
       "   time_f2  evolution_label  \n",
       "0        3                4  \n",
       "1        7                9  \n",
       "\n",
       "[2 rows x 28 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster_size</th>\n      <th>cluster_variance</th>\n      <th>cluster_density</th>\n      <th>cluster_import1</th>\n      <th>cluster_import2</th>\n      <th>cluster_range</th>\n      <th>cluster_center</th>\n      <th>time_f1</th>\n      <th>time_f2</th>\n      <th>cluster_size</th>\n      <th>...</th>\n      <th>cluster_size</th>\n      <th>cluster_variance</th>\n      <th>cluster_density</th>\n      <th>cluster_import1</th>\n      <th>cluster_import2</th>\n      <th>cluster_range</th>\n      <th>cluster_center</th>\n      <th>time_f1</th>\n      <th>time_f2</th>\n      <th>evolution_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 28 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "p = [[(1,1,1,1,1,1,1,(1,1)), (2,2,2,2,2,2,2,(2,2)), (3,3,3,3,3,3,3,(3,3)), 4], [(1,1,1,7,1,1,1,(1,1)), (2,2,2,7,2,2,2,(2,2)), (3,3,3,3,3,7,3,(3,7)), 9]]\n",
    "convert_metrics_data_to_dataframe(p)\n"
   ]
  },
  {
   "source": [
    "## Balancing and Training/Testing Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import collections\n",
    "import statistics as stat\n",
    "\n",
    "def balance_dataset(df: DataFrame) -> DataFrame:\n",
    "    # TODO\n",
    "    return data\n",
    "\n",
    "def save_training_data(layer_name: str, test_dataset_frac: float = .2) -> '(X_train, Y_train, X_test, Y_test)':\n",
    "    # load metrics data from disk\n",
    "    data: Iterable = create_metrics_training_data(layer_name=layer_name)\n",
    "    \n",
    "    # convert to df\n",
    "    df: DataFrame = convert_metrics_data_to_dataframe(data)\n",
    "    df = balance_dataset(df)\n",
    "    \n",
    "    df.to_csv(f'output/cluster_metrics/data/{layer_name}')\n",
    "\n",
    "    # # split in training and test set\n",
    "    # test_size = int(X.shape[0] * test_dataset_frac) \n",
    "    # X_train = X[test_size:]\n",
    "    # Y_train = Y[test_size:]\n",
    "    # X_test = X[:test_size]\n",
    "    # Y_test = Y[:test_size]\n",
    "\n",
    "    # print(f\"\\nWorking with: {X_train.shape[0]} training points + {X_test.shape[0]} test points ({X_test.shape[0]/(X_train.shape[0]+X_test.shape[0])}).\")\n",
    "    # print(f\"Label Occurrences: Total = {collections.Counter(Y_train.tolist() + Y_test.tolist())}, \"\\\n",
    "    #       f\"Training = {collections.Counter(Y_train)}, Test = {collections.Counter(Y_test)}\")\n",
    "    # try:\n",
    "    #     print(f\"Label Majority Class: Training = {stat.mode(Y_train)}, Test = {stat.mode(Y_test)}\\n\")\n",
    "    # except stat.StatisticsError:\n",
    "    #     print(f\"Label Majority Class: no unique mode; found 2 equally common values\")\n",
    "\n",
    "    # return X_train, Y_train, X_test, Y_test"
   ]
  }
 ]
}