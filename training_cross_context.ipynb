{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0ac79ad19892b6e891e8d97ca5fdbb2e2457e6e4ba8b10fb20aa9e37280e031f3",
   "display_name": "Python 3.7.8 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac79ad19892b6e891e8d97ca5fdbb2e2457e6e4ba8b10fb20aa9e37280e031f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Cross-Context Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_case = 'youtube'\n",
    "layer_name = 'DislikesLayer' \n",
    "reference_layer_name = 'ViewsLayer'\n",
    "\n",
    "approach = 'cross_context'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "df: DataFrame = pd.read_csv(f'data/{use_case}/ml_input/cross_context/{layer_name}_{reference_layer_name}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nWorking with: 150576 training points + 37644 test points (0.2 test ratio).\nLabel Occurrences: Total = Counter({-1.0: 92785, 3.0: 28481, 4.0: 28129, 1.0: 15471, 2.0: 14911, 0.0: 8443}), \n\tTraining = Counter({-1.0: 74172, 3.0: 22831, 4.0: 22576, 1.0: 12339, 2.0: 11941, 0.0: 6717}), \n\tTest = Counter({-1.0: 18613, 3.0: 5650, 4.0: 5553, 1.0: 3132, 2.0: 2970, 0.0: 1726})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def split_data(dataframe, test_dataset_frac=.2, shuffle=False) -> '(training_data, test_data)':\n",
    "    if shuffle:\n",
    "        dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    training_size = int(len(dataframe) * (1-test_dataset_frac))\n",
    "\n",
    "    train = dataframe[:training_size].reset_index(drop=True)\n",
    "    test = dataframe[training_size:].reset_index(drop=True)\n",
    "\n",
    "    y_train = train[train.columns[-1]]\n",
    "    y_test = test[test.columns[-1]]\n",
    "  \n",
    "    print(f\"\\nWorking with: {len(train)} training points + {len(test)} test points ({len(test)/(len(test)+len(train))} test ratio).\")\n",
    "    print(f\"Label Occurrences: Total = {collections.Counter(y_train.tolist() + y_test.tolist())}, \\n\"\\\n",
    "          f\"\\tTraining = {collections.Counter(y_train)}, \\n\"\\\n",
    "              f\"\\tTest = {collections.Counter(y_test)}\")\n",
    "    # try:\n",
    "    #     print(f\"Label Majority Class: Training = {stat.mode(Y_train)}, Test = {stat.mode(Y_test)}\\n\")\n",
    "    # except stat.StatisticsError:\n",
    "    #     print(f\"Label Majority Class: no unique mode; found 2 equally common values\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "training, testing = split_data(df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        n_nodes  n_clusters    entropy  sizes_min  sizes_max  sizes_avg  \\\n",
       "0        9086.0      7014.0  12.628675        1.0        8.0   1.295409   \n",
       "1        9162.0      7015.0  12.624013        1.0        9.0   1.306058   \n",
       "2       10040.0      6569.0  12.472724        1.0       15.0   1.528391   \n",
       "3       10180.0      6946.0  12.566904        1.0       12.0   1.465592   \n",
       "4        8939.0      6733.0  12.565393        1.0        9.0   1.327640   \n",
       "...         ...         ...        ...        ...        ...        ...   \n",
       "484987   9745.0      7290.0  12.678274        1.0       12.0   1.336763   \n",
       "484988   8986.0      7100.0  12.665515        1.0        7.0   1.265634   \n",
       "484989   9810.0      7368.0  12.686830        1.0       11.0   1.331433   \n",
       "484990  10292.0      6955.0  12.573513        1.0       11.0   1.479799   \n",
       "484991   9008.0      6898.0  12.602722        1.0        9.0   1.305886   \n",
       "\n",
       "        sizes_sum  relative_sizes_min  relative_sizes_max  relative_sizes_avg  \\\n",
       "0          9086.0            0.000110            0.000880            0.000143   \n",
       "1          9162.0            0.000109            0.000982            0.000143   \n",
       "2         10040.0            0.000100            0.001494            0.000152   \n",
       "3         10180.0            0.000098            0.001179            0.000144   \n",
       "4          8939.0            0.000112            0.001007            0.000149   \n",
       "...           ...                 ...                 ...                 ...   \n",
       "484987     9745.0            0.000103            0.001231            0.000137   \n",
       "484988     8986.0            0.000111            0.000779            0.000141   \n",
       "484989     9810.0            0.000102            0.001121            0.000136   \n",
       "484990    10292.0            0.000097            0.001069            0.000144   \n",
       "484991     9008.0            0.000111            0.000999            0.000145   \n",
       "\n",
       "        ...  relative_sizes_avg.1  relative_sizes_sum.1  center_dist_min.1  \\\n",
       "0       ...              0.000146                   1.0                0.0   \n",
       "1       ...              0.000142                   1.0                0.0   \n",
       "2       ...              0.000144                   1.0                0.0   \n",
       "3       ...              0.000307                   1.0                0.0   \n",
       "4       ...              0.000145                   1.0                0.0   \n",
       "...     ...                   ...                   ...                ...   \n",
       "484987  ...              0.000137                   1.0                0.0   \n",
       "484988  ...              0.000143                   1.0                0.0   \n",
       "484989  ...              0.000136                   1.0                0.0   \n",
       "484990  ...              0.000143                   1.0                0.0   \n",
       "484991  ...              0.000143                   1.0                0.0   \n",
       "\n",
       "        center_dist_max.1  center_dist_avg.1  center_dist_sum.1  time_f1.1  \\\n",
       "0            1.623864e+06         398.484073       2.723639e+06  -0.568065   \n",
       "1            1.002264e+06         257.336409       1.810876e+06  -0.120537   \n",
       "2            1.961570e+06        1696.761731       1.180098e+07   0.568065   \n",
       "3            2.364056e+05         369.147185       1.201943e+06   0.239316   \n",
       "4            3.759299e+05         249.743865       1.722733e+06  -0.354605   \n",
       "...                   ...                ...                ...        ...   \n",
       "484987       3.185698e+06        1957.954065       1.433418e+07   0.992709   \n",
       "484988       6.851683e+04          95.392507       6.659351e+05   0.120537   \n",
       "484989       2.643188e+06        1598.417071       1.178193e+07   0.822984   \n",
       "484990       1.531671e+06        1005.152850       7.038080e+06   0.464723   \n",
       "484991       7.701878e+05         393.751417       2.762166e+06  -0.239316   \n",
       "\n",
       "        time_f2.1  cluster_id  evolution_label  \n",
       "0        0.822984      8982.0              0.0  \n",
       "1        0.992709      6696.0              0.0  \n",
       "2       -0.822984     19590.0              2.0  \n",
       "3       -0.970942     17205.0              3.0  \n",
       "4        0.935016     11094.0              4.0  \n",
       "...           ...         ...              ...  \n",
       "484987   0.120537      1090.0              0.0  \n",
       "484988   0.992709      1920.0              0.0  \n",
       "484989   0.568065     11539.0              0.0  \n",
       "484990  -0.885456     14745.0              4.0  \n",
       "484991   0.970942      3013.0              4.0  \n",
       "\n",
       "[484992 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_nodes</th>\n      <th>n_clusters</th>\n      <th>entropy</th>\n      <th>sizes_min</th>\n      <th>sizes_max</th>\n      <th>sizes_avg</th>\n      <th>sizes_sum</th>\n      <th>relative_sizes_min</th>\n      <th>relative_sizes_max</th>\n      <th>relative_sizes_avg</th>\n      <th>...</th>\n      <th>relative_sizes_avg.1</th>\n      <th>relative_sizes_sum.1</th>\n      <th>center_dist_min.1</th>\n      <th>center_dist_max.1</th>\n      <th>center_dist_avg.1</th>\n      <th>center_dist_sum.1</th>\n      <th>time_f1.1</th>\n      <th>time_f2.1</th>\n      <th>cluster_id</th>\n      <th>evolution_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9086.0</td>\n      <td>7014.0</td>\n      <td>12.628675</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>1.295409</td>\n      <td>9086.0</td>\n      <td>0.000110</td>\n      <td>0.000880</td>\n      <td>0.000143</td>\n      <td>...</td>\n      <td>0.000146</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.623864e+06</td>\n      <td>398.484073</td>\n      <td>2.723639e+06</td>\n      <td>-0.568065</td>\n      <td>0.822984</td>\n      <td>8982.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9162.0</td>\n      <td>7015.0</td>\n      <td>12.624013</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.306058</td>\n      <td>9162.0</td>\n      <td>0.000109</td>\n      <td>0.000982</td>\n      <td>0.000143</td>\n      <td>...</td>\n      <td>0.000142</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.002264e+06</td>\n      <td>257.336409</td>\n      <td>1.810876e+06</td>\n      <td>-0.120537</td>\n      <td>0.992709</td>\n      <td>6696.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10040.0</td>\n      <td>6569.0</td>\n      <td>12.472724</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>1.528391</td>\n      <td>10040.0</td>\n      <td>0.000100</td>\n      <td>0.001494</td>\n      <td>0.000152</td>\n      <td>...</td>\n      <td>0.000144</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.961570e+06</td>\n      <td>1696.761731</td>\n      <td>1.180098e+07</td>\n      <td>0.568065</td>\n      <td>-0.822984</td>\n      <td>19590.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10180.0</td>\n      <td>6946.0</td>\n      <td>12.566904</td>\n      <td>1.0</td>\n      <td>12.0</td>\n      <td>1.465592</td>\n      <td>10180.0</td>\n      <td>0.000098</td>\n      <td>0.001179</td>\n      <td>0.000144</td>\n      <td>...</td>\n      <td>0.000307</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.364056e+05</td>\n      <td>369.147185</td>\n      <td>1.201943e+06</td>\n      <td>0.239316</td>\n      <td>-0.970942</td>\n      <td>17205.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8939.0</td>\n      <td>6733.0</td>\n      <td>12.565393</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.327640</td>\n      <td>8939.0</td>\n      <td>0.000112</td>\n      <td>0.001007</td>\n      <td>0.000149</td>\n      <td>...</td>\n      <td>0.000145</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.759299e+05</td>\n      <td>249.743865</td>\n      <td>1.722733e+06</td>\n      <td>-0.354605</td>\n      <td>0.935016</td>\n      <td>11094.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>484987</th>\n      <td>9745.0</td>\n      <td>7290.0</td>\n      <td>12.678274</td>\n      <td>1.0</td>\n      <td>12.0</td>\n      <td>1.336763</td>\n      <td>9745.0</td>\n      <td>0.000103</td>\n      <td>0.001231</td>\n      <td>0.000137</td>\n      <td>...</td>\n      <td>0.000137</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.185698e+06</td>\n      <td>1957.954065</td>\n      <td>1.433418e+07</td>\n      <td>0.992709</td>\n      <td>0.120537</td>\n      <td>1090.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>484988</th>\n      <td>8986.0</td>\n      <td>7100.0</td>\n      <td>12.665515</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.265634</td>\n      <td>8986.0</td>\n      <td>0.000111</td>\n      <td>0.000779</td>\n      <td>0.000141</td>\n      <td>...</td>\n      <td>0.000143</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>6.851683e+04</td>\n      <td>95.392507</td>\n      <td>6.659351e+05</td>\n      <td>0.120537</td>\n      <td>0.992709</td>\n      <td>1920.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>484989</th>\n      <td>9810.0</td>\n      <td>7368.0</td>\n      <td>12.686830</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>1.331433</td>\n      <td>9810.0</td>\n      <td>0.000102</td>\n      <td>0.001121</td>\n      <td>0.000136</td>\n      <td>...</td>\n      <td>0.000136</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.643188e+06</td>\n      <td>1598.417071</td>\n      <td>1.178193e+07</td>\n      <td>0.822984</td>\n      <td>0.568065</td>\n      <td>11539.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>484990</th>\n      <td>10292.0</td>\n      <td>6955.0</td>\n      <td>12.573513</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>1.479799</td>\n      <td>10292.0</td>\n      <td>0.000097</td>\n      <td>0.001069</td>\n      <td>0.000144</td>\n      <td>...</td>\n      <td>0.000143</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.531671e+06</td>\n      <td>1005.152850</td>\n      <td>7.038080e+06</td>\n      <td>0.464723</td>\n      <td>-0.885456</td>\n      <td>14745.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>484991</th>\n      <td>9008.0</td>\n      <td>6898.0</td>\n      <td>12.602722</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>1.305886</td>\n      <td>9008.0</td>\n      <td>0.000111</td>\n      <td>0.000999</td>\n      <td>0.000145</td>\n      <td>...</td>\n      <td>0.000143</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7.701878e+05</td>\n      <td>393.751417</td>\n      <td>2.762166e+06</td>\n      <td>-0.239316</td>\n      <td>0.970942</td>\n      <td>3013.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>484992 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_community_class(df):\n",
    "    '''Removes evolution_label -1 from dataset indicating the community stays empty.'''\n",
    "    # res = df.loc[df['evolution_label'] != -1.0]\n",
    "    # res = res.reset_index(drop=True)\n",
    "    # return res\n",
    "    df['evolution_label'] = df['evolution_label'].replace(-1.0, 0)\n",
    "    return df\n",
    "\n",
    "training = remove_empty_community_class(training)\n",
    "testing = remove_empty_community_class(testing)"
   ]
  },
  {
   "source": [
    "## Standardization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X = scaler.fit_transform(training)[:,:-1] # all except y\n",
    "train_Y = training[training.columns[-1]]\n",
    "\n",
    "test_X = scaler.transform(testing)[:,:-1] # all except y\n",
    "test_Y = testing[testing.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=train_X, columns=df.columns[:-1])"
   ]
  },
  {
   "source": [
    "## Balancing of Training Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    80889\n",
       "3.0    22831\n",
       "4.0    22576\n",
       "1.0    12339\n",
       "2.0    11941\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "train_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import DataSampler\n",
    "\n",
    "sampler = DataSampler()\n",
    "train_X, train_Y = sampler.sample_median_size(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    78675\n",
       "3.0    78675\n",
       "2.0    78675\n",
       "4.0    78675\n",
       "1.0    78675\n",
       "Name: evolution_label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "train_Y.value_counts()"
   ]
  },
  {
   "source": [
    "## Principal Components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "\n",
    "train_Xp = pca.fit_transform(train_X)\n",
    "test_Xp = pca.transform(test_X)"
   ]
  },
  {
   "source": [
    "## Evaluation Reports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def print_report(clfs: list, test_Xs: list, test_Y: 'y', titles: list):\n",
    "    \"\"\"\n",
    "    Prints all reports.\n",
    "    :param clfs: list of classifiers to evaluate\n",
    "    :param test_Xs: list of test_X for the corresponding classifier at idx\n",
    "    :param test_Y: true classes\n",
    "    :param titles: list of titles for the classifiers at idx\n",
    "    \"\"\"\n",
    "    for clf, test_X, title in zip(clfs, test_Xs, titles):\n",
    "        pred_Y = clf.predict(test_X)        \n",
    "        print(f\"### {title} ###\\n\", sklearn.metrics.classification_report(y_true=test_Y, y_pred=pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def export_model(model, model_name):\n",
    "    fpath = f'data/{use_case}/ml_output/{approach}/{layer_name}'\n",
    "    Path(fpath).mkdir(parents=True, exist_ok=True)\n",
    "    with open(f'{fpath}/{layer_name}_{reference_layer_name}_{model_name}.model', 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "source": [
    "# Naive Bayes\n",
    "Working best with _Xp_\n",
    "\n",
    "Parameters: \n",
    "- priors: _prior probabilities of classes_, None\n",
    "- var\\_smoothing: \\[0 , 1\\], _1E-9_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.60      0.64     20339\n",
      "         1.0       0.24      0.25      0.25      3132\n",
      "         2.0       0.19      0.71      0.30      2970\n",
      "         3.0       0.20      0.09      0.12      5650\n",
      "         4.0       0.18      0.11      0.14      5553\n",
      "\n",
      "    accuracy                           0.43     37644\n",
      "   macro avg       0.30      0.35      0.29     37644\n",
      "weighted avg       0.47      0.43      0.43     37644\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.86      0.74     20339\n",
      "         1.0       0.28      0.70      0.40      3132\n",
      "         2.0       0.26      0.21      0.23      2970\n",
      "         3.0       0.00      0.00      0.00      5650\n",
      "         4.0       0.24      0.02      0.04      5553\n",
      "\n",
      "    accuracy                           0.54     37644\n",
      "   macro avg       0.29      0.36      0.28     37644\n",
      "weighted avg       0.43      0.54      0.46     37644\n",
      "\n",
      "c:\\Users\\alexander\\Projects\\community-detection\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\alexander\\Projects\\community-detection\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\alexander\\Projects\\community-detection\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "priors = np.array([8,2,2,1,1]) / (8+2+2+1+1)\n",
    "smoothing = 1E-9\n",
    "\n",
    "clf = GaussianNB(priors=priors, var_smoothing=smoothing)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "clf_p = GaussianNB(priors=priors, var_smoothing=smoothing)\n",
    "clf_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([clf, clf_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(clf, 'nb_x')\n",
    "export_model(clf_p, 'nb_xp')"
   ]
  },
  {
   "source": [
    "# Support Vector Machine\n",
    "Parameters:\n",
    "- C (regularization): <1, _1_, >1\n",
    "- kernel: _linear_, rbf, poly, sigmoid\n",
    "- gamma (for rbf, poly, sigmoid): scale, auto, float, def=scale\n",
    "- class\\_weight: _None_, balanced, dict, def=None"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "c = 10\n",
    "kernel = 'linear'\n",
    "gamma = 'scale'\n",
    "weights = None\n",
    "\n",
    "svc = SVC(C=c, kernel=kernel, gamma=gamma, class_weight=weights)\n",
    "svc.fit(train_X, train_Y)\n",
    "\n",
    "svc_p = SVC(C=c, kernel=kernel, gamma=gamma, class_weight=weights)\n",
    "svc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([svc, svc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(svc, 'svc_x')\n",
    "export_model(svc_p, 'svc_xp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.56      0.64     20339\n",
      "         1.0       0.28      0.35      0.31      3132\n",
      "         2.0       0.27      0.62      0.37      2970\n",
      "         3.0       0.22      0.12      0.16      5650\n",
      "         4.0       0.22      0.32      0.26      5553\n",
      "\n",
      "    accuracy                           0.45     37644\n",
      "   macro avg       0.34      0.39      0.35     37644\n",
      "weighted avg       0.51      0.45      0.46     37644\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.57      0.64     20339\n",
      "         1.0       0.29      0.32      0.30      3132\n",
      "         2.0       0.26      0.64      0.37      2970\n",
      "         3.0       0.22      0.14      0.17      5650\n",
      "         4.0       0.21      0.29      0.24      5553\n",
      "\n",
      "    accuracy                           0.45     37644\n",
      "   macro avg       0.34      0.39      0.35     37644\n",
      "weighted avg       0.50      0.45      0.46     37644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_report([svc, svc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])"
   ]
  },
  {
   "source": [
    "# K-nearest Neighbors\n",
    "Parameters:\n",
    "- n\\_neighbors: _30_\n",
    "- weights: _uniform_, distance\n",
    "- algorithm: _auto_, ball_tree, kd_tree, brute\n",
    "- leaf\\_size: _50_ (no difference)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.53      0.62     20339\n",
      "         1.0       0.35      0.46      0.40      3132\n",
      "         2.0       0.33      0.44      0.38      2970\n",
      "         3.0       0.22      0.30      0.25      5650\n",
      "         4.0       0.22      0.28      0.24      5553\n",
      "\n",
      "    accuracy                           0.45     37644\n",
      "   macro avg       0.37      0.40      0.38     37644\n",
      "weighted avg       0.52      0.45      0.47     37644\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.53      0.62     20339\n",
      "         1.0       0.35      0.46      0.40      3132\n",
      "         2.0       0.33      0.44      0.38      2970\n",
      "         3.0       0.22      0.30      0.25      5650\n",
      "         4.0       0.22      0.28      0.24      5553\n",
      "\n",
      "    accuracy                           0.45     37644\n",
      "   macro avg       0.37      0.40      0.38     37644\n",
      "weighted avg       0.52      0.45      0.47     37644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_neighbors = 30\n",
    "weights = 'uniform'\n",
    "algo = 'auto'\n",
    "leaf_size = 50\n",
    "\n",
    "knnc = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algo, leaf_size=leaf_size)\n",
    "knnc.fit(train_X, train_Y)\n",
    "\n",
    "knnc_p = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights,  algorithm=algo, leaf_size=leaf_size)\n",
    "knnc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([knnc, knnc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(knnc, 'knn_x')\n",
    "export_model(knnc_p, 'knn_xp')"
   ]
  },
  {
   "source": [
    "# Decision Tree\n",
    "Working well with _Xp_\n",
    "\n",
    "Parameters:\n",
    "- criterion: _gini_, entropy\n",
    "- splitter: best, _random_\n",
    "- max\\_depth: default=_None_\n",
    "- min\\_samples\\_leaf (to construct leaf): _2_, default=1\n",
    "- min\\_impurity\\_decrease (split if the impurity is then decreased by): _1E-5_, default=0\n",
    "- ccp\\_alpha (max allowed cost after pruning): _0_, default=0 ie. nopruning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n               precision    recall  f1-score   support\n\n         0.0       0.74      0.58      0.65     20339\n         1.0       0.37      0.44      0.40      3132\n         2.0       0.35      0.44      0.39      2970\n         3.0       0.22      0.29      0.25      5650\n         4.0       0.23      0.28      0.25      5553\n\n    accuracy                           0.47     37644\n   macro avg       0.38      0.41      0.39     37644\nweighted avg       0.53      0.47      0.49     37644\n\n### Xp ###\n               precision    recall  f1-score   support\n\n         0.0       0.74      0.58      0.65     20339\n         1.0       0.37      0.38      0.37      3132\n         2.0       0.34      0.48      0.40      2970\n         3.0       0.23      0.26      0.24      5650\n         4.0       0.22      0.32      0.26      5553\n\n    accuracy                           0.47     37644\n   macro avg       0.38      0.40      0.39     37644\nweighted avg       0.52      0.47      0.49     37644\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "criterion = 'gini'\n",
    "splitter = 'random'\n",
    "max_depth = None\n",
    "min_samples_leaf = 2\n",
    "min_impurity_decrease = 1E-5 # impurity improvement needed to split\n",
    "ccp_alpha = 0\n",
    "\n",
    "seed=42\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, ccp_alpha=ccp_alpha, random_state=seed)\n",
    "dtc.fit(train_X, train_Y)\n",
    "\n",
    "dtc_p = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, ccp_alpha=ccp_alpha, random_state=seed)\n",
    "dtc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([dtc, dtc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(dtc, 'dt_x')\n",
    "export_model(dtc_p, 'dt_xp')"
   ]
  },
  {
   "source": [
    "# Random Forest\n",
    "Parameters:\n",
    "- n\\_estimators: _100_ def=100\n",
    "- criterion: _gini_, entropy\n",
    "- max\\_depth: _None_ def=None\n",
    "- min\\_samples\\_leaf (to construct leaf): _2_ def=1\n",
    "- min\\_impurity\\_decrease (split if the impurity is then decreased by): _1E-5_ default=0\n",
    "- bootstrap (if bootstraped sample is used): _True_ def=True"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.46      0.56     20339\n",
      "         1.0       0.34      0.42      0.37      3132\n",
      "         2.0       0.33      0.45      0.38      2970\n",
      "         3.0       0.21      0.31      0.25      5650\n",
      "         4.0       0.20      0.31      0.24      5553\n",
      "\n",
      "    accuracy                           0.41     37644\n",
      "   macro avg       0.36      0.39      0.36     37644\n",
      "weighted avg       0.51      0.41      0.44     37644\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.43      0.54     20339\n",
      "         1.0       0.34      0.42      0.37      3132\n",
      "         2.0       0.32      0.43      0.36      2970\n",
      "         3.0       0.20      0.32      0.25      5650\n",
      "         4.0       0.20      0.31      0.24      5553\n",
      "\n",
      "    accuracy                           0.40     37644\n",
      "   macro avg       0.36      0.38      0.35     37644\n",
      "weighted avg       0.50      0.40      0.43     37644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = 50\n",
    "criterion = 'gini'\n",
    "max_depth = None\n",
    "min_samples_leaf = 2\n",
    "min_impurity_decrease= 1E-5\n",
    "bootstrap=True\n",
    "\n",
    "seed=42\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, bootstrap=bootstrap, random_state=seed)\n",
    "rfc.fit(train_X, train_Y)\n",
    "\n",
    "rfc_p = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_impurity_decrease=min_impurity_decrease, bootstrap=bootstrap, random_state=seed)\n",
    "rfc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([rfc, rfc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(rfc, 'rf_x')\n",
    "export_model(rfc_p, 'rf_xp')"
   ]
  },
  {
   "source": [
    "# Boosting\n",
    "50% accuracy, 51% f1\n",
    "\n",
    "Parameters:\n",
    "- base\\_estimator: None\n",
    "- n\\_estimators: 50\n",
    "- algorithm: samme.r\n",
    "- learning\\_rate: .3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.67      0.70     20339\n",
      "         1.0       0.45      0.50      0.47      3132\n",
      "         2.0       0.29      0.38      0.33      2970\n",
      "         3.0       0.24      0.16      0.19      5650\n",
      "         4.0       0.24      0.33      0.28      5553\n",
      "\n",
      "    accuracy                           0.51     37644\n",
      "   macro avg       0.39      0.41      0.39     37644\n",
      "weighted avg       0.52      0.51      0.51     37644\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.67      0.70     20339\n",
      "         1.0       0.37      0.46      0.41      3132\n",
      "         2.0       0.31      0.40      0.35      2970\n",
      "         3.0       0.22      0.16      0.19      5650\n",
      "         4.0       0.23      0.30      0.26      5553\n",
      "\n",
      "    accuracy                           0.50     37644\n",
      "   macro avg       0.37      0.40      0.38     37644\n",
      "weighted avg       0.51      0.50      0.50     37644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_estimator = None# SVC(kernel='linear')\n",
    "n_estimators= 50\n",
    "algo = 'SAMME.R'\n",
    "learning_rate = .3\n",
    "\n",
    "bc = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=n_estimators, algorithm=algo, learning_rate=learning_rate)\n",
    "bc.fit(train_X, train_Y)\n",
    "\n",
    "bc_p = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=n_estimators, algorithm=algo, learning_rate=learning_rate)\n",
    "bc_p.fit(train_Xp, train_Y)\n",
    "\n",
    "print_report([bc, bc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])\n",
    "\n",
    "export_model(bc, 'boost_x')\n",
    "export_model(bc_p, 'boost_xp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### X ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.66      0.69     20207\n",
      "         1.0       0.45      0.55      0.49      3112\n",
      "         2.0       0.31      0.34      0.32      2975\n",
      "         3.0       0.26      0.17      0.21      5745\n",
      "         4.0       0.23      0.33      0.27      5605\n",
      "\n",
      "    accuracy                           0.50     37644\n",
      "   macro avg       0.39      0.41      0.40     37644\n",
      "weighted avg       0.52      0.50      0.51     37644\n",
      "\n",
      "### Xp ###\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.66      0.69     20207\n",
      "         1.0       0.41      0.46      0.43      3112\n",
      "         2.0       0.33      0.43      0.37      2975\n",
      "         3.0       0.28      0.08      0.13      5745\n",
      "         4.0       0.22      0.40      0.29      5605\n",
      "\n",
      "    accuracy                           0.50     37644\n",
      "   macro avg       0.39      0.41      0.38     37644\n",
      "weighted avg       0.52      0.50      0.50     37644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_report([bc, bc_p], [test_X, test_Xp], test_Y, [\"X\", \"Xp\"])"
   ]
  }
 ]
}