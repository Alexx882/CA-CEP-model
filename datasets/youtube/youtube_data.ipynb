{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0ac79ad19892b6e891e8d97ca5fdbb2e2457e6e4ba8b10fb20aa9e37280e031f3",
   "display_name": "Python 3.7.8  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac79ad19892b6e891e8d97ca5fdbb2e2457e6e4ba8b10fb20aa9e37280e031f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Youtube trends \n",
    "https://www.kaggle.com/datasnaek/youtube-new"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = ['CA', 'DE', 'FR', 'GB', 'IN', 'JP', 'KR', 'MX', 'RU', 'US']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "data = {}\n",
    "\n",
    "for country_code in country_codes:\n",
    "    # load videos per country\n",
    "    print(country_code)\n",
    "\n",
    "\n",
    "    category_fn = f'raw/{country_code}_category_id.json'\n",
    "\n",
    "    with open(category_fn, 'r') as f:\n",
    "        cats = json.loads(f.read())\n",
    "        categories[country_code] = {entry['id'] : entry['snippet']['title'] for entry in cats['items']}\n",
    "\n",
    "    \n",
    "    video_fn = f'raw/{country_code}videos.csv'\n",
    "\n",
    "    try:\n",
    "        data[country_code]: DataFrame = pd.read_csv(video_fn, encoding='utf-8') \n",
    "    except Exception as e:\n",
    "        print(f\"Error for {country_code} : {e}\")\n"
   ]
  },
  {
   "source": [
    "## Prepare categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one large map for all global categories as they are just \n",
    "global_cats = {}\n",
    "\n",
    "for _, cats in categories.items():\n",
    "    for cat in cats.items():\n",
    "        if cat[0] not in global_cats: \n",
    "            global_cats[cat[0]] = []\n",
    "\n",
    "        global_cats[cat[0]].append(cat[1])\n",
    "\n",
    "assert len({key : len(set(val)) for key, val in global_cats.items() if len(set(val)) > 1}) == 0, 'NOT unique and same names for categories for all countries'\n",
    "\n",
    "global_categories = {key : val[0] for key, val in global_cats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories.json', 'w') as file:\n",
    "    file.write(json.dumps(global_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_categories"
   ]
  },
  {
   "source": [
    "## Prepare data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country info and category names\n",
    "for i, cc in enumerate(country_codes):\n",
    "    data[cc]['country_code'] = cc\n",
    "    data[cc]['country_id'] = i\n",
    "\n",
    "    data[cc]['category_name'] = data[cc].apply(lambda row: global_categories[str(row['category_id'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for cc in country_codes:\n",
    "    columns = data[cc].columns\n",
    "    print(f\"{cc} ({len(columns)}) ({data[cc].size}): {columns}\")\n",
    "    sum_ += len(data[cc])\n",
    "\n",
    "print(sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(list(data.values()), ignore_index=True, join='inner')\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def convert_to_unix_timestamp(input_str: str, input_format: str) -> int:\n",
    "    '''Converts strings in form input_format to unix timestamp.'''\n",
    "    return int(time.mktime(datetime.datetime.strptime(input_str, input_format).timetuple()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['trending_timestamp'] = dfs.apply(lambda row: convert_to_unix_timestamp(row['trending_date'], input_format=\"%y.%d.%m\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['publish_timestamp'] = dfs.apply(lambda row: convert_to_unix_timestamp(row['publish_time'], input_format=\"%Y-%m-%dT%H:%M:%S.000Z\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['trend_duration'] = dfs.apply(\n",
    "    lambda row: (datetime.datetime.fromtimestamp(row['trending_timestamp']).date() - datetime.datetime.fromtimestamp(row['publish_timestamp']).date()).days   \n",
    "    , axis=1)"
   ]
  },
  {
   "source": [
    "dfs.to_csv('videos.csv')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Upload preprocessed data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "dfs: DataFrame = pd.read_csv('videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "jwt = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6InJlZ3VsYXJAaXRlYy5hYXUuYXQiLCJjcmVhdGVkX2F0IjoiMjAyMS0wNS0wNCAxMjoyNzo1Ny4yOTQxNzMiLCJ2YWxpZF91bnRpbCI6IjIwMjEtMDUtMDUgMTI6Mjc6NTcuMjk0MTczIn0.Mdvi-dy_PshPfoqujIcKzJLux-g3pMPfhM2ZmP6JeBY\"\n",
    "\n",
    "\n",
    "def send_transaction_to_rest_gateway(transaction: dict):\n",
    "    res = requests.post(\n",
    "        url = 'https://articonf1.itec.aau.at:30401/api/trace',\n",
    "        json = transaction,\n",
    "        headers = {\"Authorization\": f\"Bearer {jwt}\"},\n",
    "        verify = False # ignore ssl error\n",
    "    )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "use_case = 'community-prediction-youtube'\n",
    "table_name = 'community-prediction-youtube'\n",
    "     \n",
    "for idx, entry in dfs.iterrows():\n",
    "    row = entry.to_dict()\n",
    "\n",
    "    row['ApplicationType'] = use_case\n",
    "    row['docType'] = table_name\n",
    "\n",
    "    res = send_transaction_to_rest_gateway(row)\n",
    "    print(idx, res)\n",
    "    "
   ]
  }
 ]
}